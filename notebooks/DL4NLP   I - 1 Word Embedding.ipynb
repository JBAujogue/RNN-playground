{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "  Deep Learning for NLP\n",
    "  </div> \n",
    "  \n",
    "<div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 30px; \n",
    "      text-align: center; \n",
    "      padding: 15px; \n",
    "      margin: 10px;\">\n",
    "    <font color=orange>I - 1 </font>\n",
    "  Word Embedding\n",
    "  </div> \n",
    "\n",
    "  <div style=\"font-variant: small-caps; \n",
    "      font-weight: normal; \n",
    "      font-size: 20px; \n",
    "      text-align: center; \n",
    "      padding: 15px;\">\n",
    "  </div> \n",
    "\n",
    "  <div style=\" float:right; \n",
    "      font-size: 12px; \n",
    "      line-height: 12px; \n",
    "  padding: 10px 15px 8px;\">\n",
    "  Jean-baptiste AUJOGUE\n",
    "  </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I\n",
    "1. <font color=orange>**Word Embedding**</font>\n",
    "\n",
    "2. Sentence Classification\n",
    "\n",
    "3. Language Modeling\n",
    "\n",
    "4. Sequence Labelling\n",
    "\n",
    "\n",
    "### Part II\n",
    "\n",
    "1. Text Classification\n",
    "\n",
    "2. Sequence to sequence\n",
    "\n",
    "\n",
    "\n",
    "### Part III\n",
    "\n",
    "1. Abstractive Summarization\n",
    "\n",
    "2. Question Answering\n",
    "\n",
    "3. Chatbot\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The global purpose of Word Embedding is to represent a _Token_ , a raw string representing a unit of text, as a low dimensional (dense) vector. The way tokens are defined only depends on the method used to split a text into text units : using blank spaces as separators or using classical NLTK or SpaCy's segmentation models leave _words_ as tokens, but splitting protocols yielding _subword units_ , that are half-way between characters and full words, are also investigated :\n",
    "\n",
    "- [Neural Machine Translation of Rare Words with Subword Units (2015)](https://www.aclweb.org/anthology/P16-1162.pdf)\n",
    "- [Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation (2016)](https://arxiv.org/pdf/1609.08144.pdf)). \n",
    "- [BPEmb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages (2018)](https://www.aclweb.org/anthology/L18-1473.pdf)\n",
    "- [SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (2018)](https://arxiv.org/abs/1808.06226)\n",
    "\n",
    "\n",
    "Here we broadly denote by _word_ any such token. Commonly followed approaches for the embedding of words (aka tokens) decompose into three levels of granularity :\n",
    "\n",
    "| Level |  | |\n",
    "|------|------|------|\n",
    "| **Word** | [I.1 Custom model](#word_level_custom) | [I.2 Gensim Model](#gensim) |\n",
    "| **sub-word unit** | [II.1 FastText model](#fastText) |  |\n",
    "| **Character** |  |  |\n",
    "\n",
    "\n",
    "<br>\n",
    "Visualization with TensorBoard : https://www.tensorflow.org/guide/embedding (TODO)\n",
    "\n",
    "# Training objectives\n",
    "\n",
    "#### CBOW training objective\n",
    "\n",
    "Cette méthode de vectorisation est introduite dans \\cite{mikolov2013distributed, mikolov2013efficient}, et consiste à construire pour un vocabulaire de mots une table de vectorisation $T$ contenant un vecteur par mot. La spécificité de cette méthode est que cette vectorisation est faite de façon à pouvoir prédire chaque mot à partir de son contexte. La construction de cette table $T$ passe par la création d'un réseau de neurones, qui sert de modèle pour l'estimation de la probabilité de prédiction d'un mot $w_t$ d'après son contexte $c = w_{t-N}, \\, ... \\, , w_{t-1}$, $w_{t+1}, \\, ... \\, , w_{t+N}$. La table $T$ intégrée au modèle sera optimisée lorsque ce modèle sera entrainé de façon à ce qu'un mot $w_t$ maximise la vraisemblance de la probabilité $P(. \\, | \\, c)$ fournie par le modèle. \n",
    "\n",
    "Le réseau de neurones de décrit de la façon suivante :\n",
    "\n",
    "![cbow](figs/CBOW.png)\n",
    "\n",
    "Un contexte $c = w_{t-N}, \\, ... \\, , w_{t-1}$, $w_{t+1}, \\, ... \\, , w_{t+N}$ est vectorisé via une table $T$ fournissant un ensemble de vecteurs denses (typiquement de dimension comprise entre 50 et 300) $T(w_{t-N}), \\, ... \\, , T(w_{t-1})$, $T(w_{t+1}), \\, ... \\, , T(w_{t+N})$. Chaque vecteur est ensuite transformé via une transformation affine, dont les vecteurs résultants sont superposés en un unique vecteur\n",
    "\n",
    "\\begin{align*}\n",
    "v_c = \\sum _{i = - N}^N M_i T(w_{t+i}) + b_i\n",
    "\\end{align*}\n",
    "\n",
    "Le vecteur $v_c$ est de dimension typiquement égale à la dimension de la vectorisation de mots. Une autre table $T'$ est utilisée pour une nouvelle vectorisation du vocabulaire, de sorte que le mot $w_{t}$ soit transformé en un vecteur $T'(w_{t})$ par cette table, et soit proposé en position $t$ avec probabilité\n",
    "\n",
    "\\begin{align*}\n",
    "P(w_{t} \\, | \\, c\\,) = \\frac{\\exp\\left( T'(w_{t}) \\cdot v_c \\right) }{\\displaystyle \\sum _{w \\in \\mathcal{V}} \\exp\\left(   T'(w) \\cdot v_c \n",
    "\\right) }\n",
    "\\end{align*}\n",
    "\n",
    "Ici $\\cdot$ désigne le produit scalaire entre vecteurs. L'optimisation de ce modèle permet d'ajuster la table $T$ afin que les vecteurs de mots portent suffisamment d'information pour reformer un mot à partir du contexte.\n",
    "\n",
    "\n",
    "#### Skip-Gram training objective\n",
    "\n",
    "\n",
    "Cette méthode de vectorisation est introduite dans \\cite{mikolov2013distributed, mikolov2013efficient} comme version mirroir au Continuous Bag Of Words, et consiste là encore à construire pour un vocabulaire de mots une table de vectorisation $T$ contenant un vecteur par mot. La spécificité de cette méthode est que cette vectorisation est faite non pas de façon prédire un mot central $w$ à partir d'un contexte $c $ comme pour CBOW, mais plutôt de prédire le contexte $c $ à partir du mot central $w$. La construction de cette table $T$ passe par la création d'un réseau de neurones servant de modèle pour l'estimation de la probabilité de prédiction d'un contexte $c = w_{t-N}, \\, ... \\, , w_{t-1}$, $w_{t+1}, \\, ... \\, , w_{t+N}$ à partir d'un mot central $w_t$. La table $T$ intégrée au modèle sera optimisée lorsque ce modèle sera entrainé de façon à ce que le contexte  $ c $ maximise la vraisemblance de la probabilité $P( . \\, | \\, w_t)$ fournie par le modèle.\n",
    "\n",
    "\n",
    "Une implémentation de ce modèle est la suivante : \n",
    "\n",
    "\n",
    "![skipgram](figs/Skipgram.png)\n",
    "\n",
    "\n",
    "Un mot courant $w_t$ est vectorisé par une table $T$ fournissant un vecteur dense (typiquement de dimension comprise entre 50 et 300) $T(w_t)$. Ce vecteur est alors transformé en un ensemble de $2N$ vecteurs\n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma (M_{i} T(w_t) + b_{i}) \\qquad \\qquad i =-N,\\, ...\\, , -1, 1, \\, ...\\, , N\n",
    "\\end{align*}\n",
    "\n",
    "où $N$ désigne la taille de la fenêtre retenue, d'une dimension typiquement égale à la dimension de la vectorisation de mots, et $\\sigma$ une fonction non linéaire (typiquement la _Rectified Linear Unit_ $\\sigma (x) = max (0, x)$). Une autre table $T'$ est utilisée pour une nouvelle vectorisation du vocabulaire, de sorte que chaque mot $w_{t+i}$, transformé en un vecteur $T'(w_{t+i})$ par cette table, soit proposé en position $t+i$ avec probabilité\n",
    "\n",
    "\\begin{align*}\n",
    "P( w_{t+i} | \\, w_t) = \\frac{\\exp\\left(  T'(w_{t+i}) ^\\perp \\sigma \\left( M_i T(w_t) + b_{i}\\right) \\right) }{\\displaystyle \\sum _{w \\in \\mathcal{V}} \\exp\\left(   T'(w) ^\\perp \\sigma \\left( M_i T(w_t) + b_i\\right) \\right) }\n",
    "\\end{align*}\n",
    "\n",
    "On modélise alors la probabilité qu'un ensemble de mots $c = w_{t-N}, \\, ... \\, , w_{t-1}$, $w_{t+1}, \\, ... \\, , w_{t+N}$ soit le contexte d'un mot $w_t$ par le produit\n",
    "\n",
    "\\begin{align*}\n",
    " P( c\\, | \\, w_t) = \\prod _{i = -N}^N P( w_{t+i}\\, | \\, w_t)\n",
    "\\end{align*}\n",
    "\n",
    "Ce modèle de probabilité du contexte d'un mot est naif au sens où les mots de contextes sont considérés comme indépendants deux à deux dès lors que le mot central est connu. Cette approximation rend cependant le calcul d'optimisation beaucoup plus court.\n",
    "\n",
    "\n",
    "\n",
    "L'optimisation de ce modèle permet d'ajuster la table $T$ afin que les vecteurs de mots portent suffisamment d'information pour reformer l'intégralité du contexte à partir de ce seul mot. La vectorisation Skip-Gram est typiquement plus performante que CBOW, car la table $T$ subit plus de contrainte dans son optimisation, et puisque le vecteur d'un mot est obtenu de façon à pouvoir prédire l'utilisation réelle du mot, ici donnée par son contexte. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages\n",
    "\n",
    "[Back to top](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version : 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]\n",
      "pytorch version : 1.5.0\n",
      "DL device : cuda\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import sys\n",
    "import warnings\n",
    "import os\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "from unidecode import unidecode\n",
    "import multiprocessing\n",
    "\n",
    "# for special math operation\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "# for manipulating data \n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=np.nan)\n",
    "import pandas as pd\n",
    "import bcolz # see https://bcolz.readthedocs.io/en/latest/intro.html\n",
    "import pickle\n",
    "\n",
    "\n",
    "# for text processing\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import spacy\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "# for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('python version :', sys.version)\n",
    "print('pytorch version :', torch.__version__)\n",
    "print('DL device :', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_DL4NLP = os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(path_to_DL4NLP + '\\\\lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Le texte est importé et mis sous forme de liste, où chaque élément représente un texte présenté sous forme d'une chaine de caractères.<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AGnews = pd.read_csv(path_to_DL4NLP + \"\\\\data\\\\AG News\\\\train.csv\", sep = ',', header = None, error_bad_lines = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AGnews.columns = ['index', 'title', 'description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                              title  \\\n",
       "0      3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1      3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2      3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3      3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4      3  Oil prices soar to all-time record, posing new...   \n",
       "\n",
       "                                         description  \n",
       "0  Reuters - Short-sellers, Wall Street's dwindli...  \n",
       "1  Reuters - Private investment firm Carlyle Grou...  \n",
       "2  Reuters - Soaring crude prices plus worries\\ab...  \n",
       "3  Reuters - Authorities have halted oil export\\f...  \n",
       "4  AFP - Tearaway world oil prices, toppling reco...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_AGnews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit le tokeniseur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s : str) :\n",
    "    def unicodeToAscii(s):\n",
    "        return ''.join( c for c in unicodedata.normalize('NFD', s)\n",
    "                        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "    def normalizeString(s):\n",
    "        s = unicodeToAscii(s.strip())\n",
    "        return s\n",
    "\n",
    "    def cleanSentence(s) :\n",
    "        s = s.lower()\n",
    "        s = s.replace('\\\\', ' ')\n",
    "        s = re.sub('[\\.!?]+ ', ' . ', s)\n",
    "        s = s.replace('%', ' % ')\n",
    "        s = re.sub(' [0-9]*\\.[0-9]', ' FLOAT ', ' ' + s).strip()\n",
    "        s = re.sub(' [0-9,]*[0-9]', ' INT ', ' ' + s).strip()\n",
    "\n",
    "        for w in ['\"', \"'\", '”', '“', '/', '(', ')', '[', ']', '<', '>', ':', ','] : s = s.replace(w, '')\n",
    "        return s\n",
    "\n",
    "    def trueWord(w) :\n",
    "        return len(w)>0 and re.sub('[^a-zA-Z0-9.,]', '', w) != ''\n",
    "\n",
    "    # -- main --\n",
    "    s = normalizeString(s)\n",
    "    s = cleanSentence(s)\n",
    "    s = nltk.tokenize.word_tokenize(s)\n",
    "    s = [w for w in s if trueWord(w)]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [s1 + ' . ' + s2 for s1, s2 in zip(df_AGnews[\"title\"].values.tolist(), \n",
    "                                            df_AGnews[\"description\"].values.tolist()) if tokenize(s1) != []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokenized = [tokenize(s) for s in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"word_level\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1 Word Embedding\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"word_level_custom\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Custom Word-level Embedding Model\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "### 1.1.1 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language\n",
    "\n",
    "Classe de langage prennant en paramètre un corpus de la forme [[str]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from libDL4NLP.utils.Lang import Lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, corpus = None, base_tokens = ['UNK'], min_count = None):\n",
    "        self.base_tokens = base_tokens\n",
    "        self.initData(base_tokens)\n",
    "        if    corpus is not None : self.addCorpus(corpus)\n",
    "        if min_count is not None : self.removeRareWords(min_count)\n",
    "\n",
    "        \n",
    "    def initData(self, base_tokens) :\n",
    "        self.word2index = {word : i for i, word in enumerate(base_tokens)}\n",
    "        self.index2word = {i : word for i, word in enumerate(base_tokens)}\n",
    "        self.word2count = {word : 0 for word in base_tokens}\n",
    "        self.n_words = len(base_tokens)\n",
    "        return\n",
    "    \n",
    "    def getIndex(self, word) :\n",
    "        if    word in self.word2index : return self.word2index[word]\n",
    "        elif 'UNK' in self.word2index : return self.word2index['UNK']\n",
    "        return\n",
    "        \n",
    "    def addWord(self, word):\n",
    "        '''Add a word to the language'''\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "        return \n",
    "            \n",
    "    def addSentence(self, sentence):\n",
    "        '''Add to the language all words of a sentence'''\n",
    "        words = sentence if type(sentence) == list else nltk.word_tokenize(sentence)\n",
    "        for word in words : self.addWord(word)          \n",
    "        return\n",
    "            \n",
    "    def addCorpus(self, corpus):\n",
    "        '''Add to the language all words contained into a corpus'''\n",
    "        for text in corpus : self.addSentence(text)\n",
    "        return \n",
    "                \n",
    "    def removeRareWords(self, min_count):\n",
    "        '''remove words appearing lesser than a min_count threshold'''\n",
    "        kept_word2count = {word: count for word, count in self.word2count.items() if count >= min_count}\n",
    "        self.initData(self.base_tokens)\n",
    "        for word, count in kept_word2count.items(): \n",
    "            self.addWord(word)\n",
    "            self.word2count[word] = kept_word2count[word]\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveLang(name, lang):\n",
    "    with open(path_to_DL4NLP + '\\\\saves\\\\' + name + '.file', 'wb') as fil :\n",
    "        pickle.dump(lang, fil)\n",
    "    return\n",
    "\n",
    "def importLang(name):\n",
    "    with open(path_to_DL4NLP + '\\\\saves\\\\' + name + '.file', 'rb') as fil :\n",
    "        lang = pickle.load(fil)\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mots comptés avant : 84308\n",
      "Mots comptés après : 29846\n"
     ]
    }
   ],
   "source": [
    "lang = Lang(corpus_tokenized, base_tokens = ['SOS', 'EOS', 'UNK'])\n",
    "print(\"Mots comptés avant : {}\".format(lang.n_words))\n",
    "lang.removeRareWords(min_count = 5)\n",
    "print(\"Mots comptés après : {}\".format(lang.n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saveLang(name = 'DL4NLP_I1_lang', lang = lang)\n",
    "#lang = importLang(name = 'DL4NLP_I1_lang')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparaison avec un vocabulaire de référence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76\n",
    "\n",
    "# --------------------- comparison with Glove vocab ------------------------\n",
    "def vocabGlove(name, path = 'D:\\\\data\\\\vectors\\\\') :\n",
    "    words = []\n",
    "    path += name \n",
    "    with open(path + '.txt', 'rb') as f:\n",
    "        for l in f:\n",
    "            line = l.decode().split()\n",
    "            word = line[0]\n",
    "            words.append(word)\n",
    "    return words\n",
    "\n",
    "def intersection(lst1, lst2): \n",
    "    return list(set(lst1) & set(lst2))\n",
    "\n",
    "def comparaison(lang) :\n",
    "    vocab_lang = list(lang.word2index.keys())\n",
    "    intersect_glove = intersection(vocab_glove, vocab_lang)\n",
    "    reste_glove = np.setdiff1d(vocab_lang, intersect_glove)\n",
    "    printComparaison('glove', vocab_lang, intersect_glove, reste_glove)\n",
    "    return intersect_glove, reste_glove\n",
    "\n",
    "def printComparaison(nom, vocab_lang, intersect, reste) :\n",
    "    print('proportion de mots du langage appartenants à {}  {:.2f} % \\nproportion de mots du langage ny appartenant pas     {:.2f} %'.format(nom, len(intersect)*100/len(vocab_lang),len(reste)*100/len(vocab_lang) ) )\n",
    "\n",
    "\n",
    "# --------------------- detect missing spaces ------------------------\n",
    "def checkWhetherBroken(vocab, clean_vocab) :\n",
    "    exit = {}\n",
    "    for word in vocab :\n",
    "        exit[word] = True if word in clean_vocab else False\n",
    "    return exit\n",
    "\n",
    "def checkMissingSpaces(word, clean_vocab) :\n",
    "    for word2 in clean_vocab :\n",
    "        if word.startswith(word2) :\n",
    "            rest = word.replace(word2, '')\n",
    "            if rest in clean_vocab :\n",
    "                return word2 + ' ' + rest\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_glove = vocabGlove('glove.6B.100d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion de mots du langage appartenants à glove  93.37 % \n",
      "proportion de mots du langage ny appartenant pas     6.63 %\n"
     ]
    }
   ],
   "source": [
    "words_glove, reste_glove = comparaison(lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec model\n",
    "\n",
    "[Back to top](#plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from libDL4NLP.models.Word_Embedding import Word2Vec as myWord2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myWord2Vec(nn.Module) :\n",
    "    def __init__(self, lang, \n",
    "                 T = 100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lang = lang\n",
    "        if type(T) == int :\n",
    "            self.embedding = nn.Embedding(lang.n_words, T)  \n",
    "        else :\n",
    "            self.embedding = nn.Embedding(T.shape[0], T.shape[1])\n",
    "            self.embedding.weight = nn.Parameter(torch.FloatTensor(T))\n",
    "            \n",
    "        self.out_dim = self.lookupTable().shape[1]\n",
    "        self.sims = None\n",
    "        \n",
    "    def lookupTable(self) :\n",
    "        return self.embedding.weight.cpu().detach().numpy()\n",
    "        \n",
    "    def computeSimilarities(self) :\n",
    "        T = normalize(self.lookupTable(), norm = 'l2', axis = 1)\n",
    "        self.sims = np.matmul(T, T.transpose())\n",
    "        return\n",
    "\n",
    "    def most_similar(self, word, bound = 10) :\n",
    "        if word not in self.lang.word2index : return\n",
    "        if self.sims is None : self.computeSimilarities()\n",
    "        index = self.lang.word2index[word]\n",
    "        coefs = self.sims[index]\n",
    "        indices = coefs.argsort()[-bound -1 :-1]\n",
    "        output = [(self.lang.index2word[i], coefs[i]) for i in reversed(indices)]\n",
    "        return output\n",
    "    \n",
    "    def wv(self, word) :\n",
    "        return self.lookupTable()[self.lang.getIndex(word)]\n",
    "    \n",
    "    def addWord(self, word, vector = None) :\n",
    "        self.lang.addWord(word)\n",
    "        T = self.lookupTable()\n",
    "        v = np.random.rand(1, T.shape[1]) if vector is None else vector\n",
    "        updated_T = np.concatenate((T, v), axis = 0)\n",
    "        self.embedding = nn.Embedding(updated_T.shape[0], updated_T.shape[1])\n",
    "        self.embedding.weight = nn.Parameter(torch.FloatTensor(updated_T))\n",
    "        return\n",
    "    \n",
    "    def freeze(self) :\n",
    "        for param in self.embedding.parameters() : param.requires_grad = False\n",
    "        return self\n",
    "    \n",
    "    def unfreeze(self) :\n",
    "        for param in self.embedding.parameters() : param.requires_grad = True\n",
    "        return self\n",
    "    \n",
    "    def forward(self, words, device = None) :\n",
    "        '''Transforms a list of n words into a torch.FloatTensor of size (1, n, emb_dim)'''\n",
    "        indices  = [self.lang.getIndex(w) for w in words]\n",
    "        indices  = [[i for i in indices if i is not None]]\n",
    "        variable = Variable(torch.LongTensor(indices)) # size = (1, n)\n",
    "        if device is not None : variable = variable.to(device)\n",
    "        tensor   = self.embedding(variable)            # size = (1, n, emb_dim)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec Shell\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Shell acting as a wrapper around the Word2Vec model, implementing :\n",
    "\n",
    "- The layers suited for the training objective\n",
    "- The methods for all optimization steps\n",
    "- The methods for generating the data suitable for the optimization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from libDL4NLP.models.Word_Embedding import Word2VecShell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecShell(nn.Module):\n",
    "    '''Word2Vec model :\n",
    "        - sg = 0 yields CBOW training procedure\n",
    "        - sg = 1 yields Skip-Gram training procedure\n",
    "    '''\n",
    "    def __init__(self, word2vec, device, \n",
    "                 sg = 0, \n",
    "                 context_size = 5, \n",
    "                 weight_tying = True,\n",
    "                 criterion = nn.NLLLoss(size_average = False), \n",
    "                 optimizer = optim.SGD):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # core of Word2Vec\n",
    "        self.word2vec = word2vec\n",
    "        \n",
    "        # training layers\n",
    "        self.in_n_word  = (2 * context_size if sg == 0 else 1)\n",
    "        self.out_n_word = (1 if sg == 0 else 2 * context_size)\n",
    "        self.word_size  = word2vec.embedding.weight.size(1)\n",
    "        self.linear_1   = nn.Linear(self.in_n_word * self.word_size, self.out_n_word * self.word_size)\n",
    "        self.linear_2   = nn.Linear(self.word_size, word2vec.lang.n_words, bias = False)\n",
    "        \n",
    "        # weight tying\n",
    "        if weight_tying : self.linear_2.weight = self.word2vec.embedding.weight\n",
    "        \n",
    "        # training tools\n",
    "        self.sg = sg\n",
    "        self.weight_tying = weight_tying\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # load to device\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        '''Transforms a batch of Ngrams of size (batch_size, in_n_word)\n",
    "           Into log probabilities of size (batch_size, lang.n_words, out_n_word)\n",
    "           '''\n",
    "        batch = batch.to(self.device)                 # size = (batch_size, self.in_n_word)\n",
    "        embed = self.word2vec.embedding(batch)        # size = (batch_size, self.in_n_word, emb_dim)\n",
    "        embed = embed.view((batch.size(0), -1))       # size = (batch_size, self.in_n_word * emb_dim)\n",
    "        out = self.linear_1(embed)                    # size = (batch_size, self.out_n_word * hid_dim) \n",
    "        out = out.view((batch.size(0),self.out_n_word, -1))\n",
    "        if not self.weight_tying : out = F.relu(out)  # size = (batch_size, self.out_n_word, hid_dim)                                         \n",
    "        out = self.linear_2(out)                      # size = (batch_size, self.out_n_word, lang.n_words)\n",
    "        out = torch.transpose(out, 1, 2)              # size = (batch_size, lang.n_words, self.out_n_word)\n",
    "        log_probs = F.log_softmax(out, dim = 1)       # size = (batch_size, lang.n_words, self.out_n_word)\n",
    "        return log_probs\n",
    "    \n",
    "    def generatePackedNgrams(self, corpus, \n",
    "                             context_size = 5, \n",
    "                             batch_size = 32, \n",
    "                             seed = 42) :\n",
    "        # generate Ngrams\n",
    "        data = []\n",
    "        for text in corpus :\n",
    "            text = [w for w in text if w in self.word2vec.lang.word2index]\n",
    "            text = ['SOS' for i in range(context_size)] + text + ['EOS' for i in range(context_size)]\n",
    "            for i in range(context_size, len(text) - context_size):\n",
    "                context = text[i-context_size : i] + text[i+1 : i+context_size+1]\n",
    "                word = text[i]\n",
    "                data.append([word, context])\n",
    "                \n",
    "        # pack Ngrams into mini_batches\n",
    "        random.seed(seed)\n",
    "        random.shuffle(data)\n",
    "        packed_data = []\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            pack0 = [el[0] for el in data[i:i + batch_size]]\n",
    "            pack0 = [[self.word2vec.lang.getIndex(w)] for w in pack0]\n",
    "            pack0 = Variable(torch.LongTensor(pack0)) # size = (batch_size, 1)\n",
    "            pack1 = [el[1] for el in data[i:i + batch_size]]\n",
    "            pack1 = [[self.word2vec.lang.getIndex(w) for w in context] for context in pack1]\n",
    "            pack1 = Variable(torch.LongTensor(pack1)) # size = (batch_size, 2*context_size)   \n",
    "            if   self.sg == 1 : packed_data.append([pack0, pack1])\n",
    "            elif self.sg == 0 : packed_data.append([pack1, pack0])\n",
    "            else :\n",
    "                print('sg should be either 0 or 1')\n",
    "                pass\n",
    "        return packed_data\n",
    "    \n",
    "    def train(self, ngrams, \n",
    "              iters = None, \n",
    "              epochs = None, \n",
    "              lr = 0.025, \n",
    "              random_state = 42,\n",
    "              print_every = 10, \n",
    "              compute_accuracy = False):\n",
    "        \"\"\"Performs training over a given dataset and along a specified amount of loop\n",
    "        s\"\"\"\n",
    "        def asMinutes(s):\n",
    "            m = math.floor(s / 60)\n",
    "            s -= m * 60\n",
    "            return '%dm %ds' % (m, s)\n",
    "\n",
    "        def timeSince(since, percent):\n",
    "            now = time.time()\n",
    "            s = now - since\n",
    "            rs = s/percent - s\n",
    "            return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "        def computeAccuracy(log_probs, targets) :\n",
    "            accuracy = 0\n",
    "            acc = sum([log_probs[i, :, j].data.topk(1)[1].item() == targets[i, j].item() \n",
    "                       for i in range(targets.size(0)) \n",
    "                       for j in range(targets.size(1))])\n",
    "            return (acc * 100) / (targets.size(0) * targets.size(1))\n",
    "\n",
    "        def printScores(start, iter, iters, tot_loss, tot_loss_words, print_every, compute_accuracy) :\n",
    "            avg_loss = tot_loss / print_every\n",
    "            avg_loss_words = tot_loss_words / print_every\n",
    "            if compute_accuracy : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}  accuracy : {:.1f} %'.format(iter, int(iter / iters * 100), avg_loss, avg_loss_words))\n",
    "            else                : print(timeSince(start, iter / iters) + ' ({} {}%) loss : {:.3f}                     '.format(iter, int(iter / iters * 100), avg_loss))\n",
    "            return 0, 0\n",
    "\n",
    "        def trainLoop(couple, optimizer, compute_accuracy = False):\n",
    "            \"\"\"Performs a training loop, with forward pass and backward pass for gradient optimisation.\"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            self.zero_grad()\n",
    "            log_probs = self(couple[0])           # size = (batch_size, agent.out_n_word, agent.lang.n_words)\n",
    "            targets   = couple[1].to(self.device) # size = (batch_size, agent.out_n_word)\n",
    "            loss      = self.criterion(log_probs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            accuracy = computeAccuracy(log_probs, targets) if compute_accuracy else 0\n",
    "            return float(loss.item() / (targets.size(0) * targets.size(1))), accuracy\n",
    "        \n",
    "        # -- main --\n",
    "        np.random.seed(random_state)\n",
    "        start = time.time()\n",
    "        optimizer = self.optimizer([param for param in self.parameters() if param.requires_grad == True], lr = lr)\n",
    "        tot_loss = 0  \n",
    "        tot_loss_words = 0\n",
    "        if epochs is None :\n",
    "            for iter in range(1, iters + 1):\n",
    "                couple = random.choice(ngrams)\n",
    "                loss, loss_words = trainLoop(couple, optimizer, compute_accuracy)\n",
    "                tot_loss += loss\n",
    "                tot_loss_words += loss_words      \n",
    "                if iter % print_every == 0 : \n",
    "                    tot_loss, tot_loss_words = printScores(start, iter, iters, tot_loss, tot_loss_words, print_every, compute_accuracy)\n",
    "        else :\n",
    "            iter = 0\n",
    "            iters = len(ngrams) * epochs\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                print('epoch ' + str(epoch))\n",
    "                np.random.shuffle(ngrams)\n",
    "                for couple in ngrams :\n",
    "                    loss, loss_words = trainLoop(couple, optimizer, compute_accuracy)\n",
    "                    tot_loss += loss\n",
    "                    tot_loss_words += loss_words \n",
    "                    iter += 1\n",
    "                    if iter % print_every == 0 : \n",
    "                        tot_loss, tot_loss_words = printScores(start, iter, iters, tot_loss, tot_loss_words, print_every, compute_accuracy)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Training with CBOW objective\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "\n",
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29846"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang = Lang(corpus_tokenized, base_tokens = ['SOS', 'EOS', 'UNK'], min_count = 5)\n",
    "lang.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cbow.word2vec = word2vec : True\n"
     ]
    }
   ],
   "source": [
    "word2vec = myWord2Vec(lang, T = 100)\n",
    "cbow = Word2VecShell(word2vec, device, sg = 0, context_size = 5, weight_tying = False)\n",
    "print('cbow.word2vec = word2vec :', cbow.word2vec == word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74874"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ngrams = cbow.generatePackedNgrams(corpus_tokenized, context_size = 5, batch_size = 64, seed = 42)\n",
    "len(Ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "\n",
    "The training methods allows to display accuracy over predicted target words. However, since the underlying computation is quite time consuming, we display accuracy only at the begining of training, and a few times periodically along the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "0m 10s (- 128m 33s) (100 0%) loss : 9.769  accuracy : 6.4 %\n",
      "0m 19s (- 124m 2s) (200 0%) loss : 9.259  accuracy : 7.7 %\n",
      "0m 30s (- 127m 17s) (300 0%) loss : 8.964  accuracy : 7.6 %\n",
      "0m 40s (- 124m 59s) (400 0%) loss : 8.751  accuracy : 8.9 %\n",
      "0m 49s (- 123m 38s) (500 0%) loss : 8.570  accuracy : 9.8 %\n",
      "1m 0s (- 124m 51s) (600 0%) loss : 8.425  accuracy : 10.5 %\n",
      "1m 9s (- 123m 35s) (700 0%) loss : 8.347  accuracy : 10.2 %\n",
      "1m 19s (- 122m 34s) (800 1%) loss : 8.220  accuracy : 10.7 %\n",
      "1m 29s (- 123m 7s) (900 1%) loss : 8.108  accuracy : 11.2 %\n",
      "1m 40s (- 123m 20s) (1000 1%) loss : 8.050  accuracy : 11.3 %\n",
      "1m 49s (- 122m 43s) (1100 1%) loss : 8.067  accuracy : 11.4 %\n",
      "1m 59s (- 122m 30s) (1200 1%) loss : 7.974  accuracy : 11.8 %\n",
      "2m 9s (- 121m 59s) (1300 1%) loss : 7.827  accuracy : 12.1 %\n",
      "2m 19s (- 121m 48s) (1400 1%) loss : 7.811  accuracy : 12.0 %\n",
      "2m 29s (- 121m 53s) (1500 2%) loss : 7.768  accuracy : 12.7 %\n",
      "2m 38s (- 121m 18s) (1600 2%) loss : 7.752  accuracy : 11.7 %\n",
      "2m 49s (- 121m 46s) (1700 2%) loss : 7.677  accuracy : 12.2 %\n",
      "3m 0s (- 121m 54s) (1800 2%) loss : 7.566  accuracy : 12.8 %\n",
      "3m 9s (- 121m 36s) (1900 2%) loss : 7.552  accuracy : 12.5 %\n",
      "3m 20s (- 121m 29s) (2000 2%) loss : 7.567  accuracy : 12.6 %\n",
      "3m 29s (- 121m 17s) (2100 2%) loss : 7.419  accuracy : 13.4 %\n",
      "3m 39s (- 120m 40s) (2200 2%) loss : 7.502  accuracy : 13.1 %\n",
      "3m 48s (- 120m 18s) (2300 3%) loss : 7.472  accuracy : 12.8 %\n",
      "3m 58s (- 120m 9s) (2400 3%) loss : 7.385  accuracy : 13.9 %\n",
      "4m 8s (- 119m 52s) (2500 3%) loss : 7.358  accuracy : 13.9 %\n",
      "4m 17s (- 119m 27s) (2600 3%) loss : 7.352  accuracy : 12.9 %\n",
      "4m 28s (- 119m 25s) (2700 3%) loss : 7.310  accuracy : 13.3 %\n",
      "4m 37s (- 119m 9s) (2800 3%) loss : 7.275  accuracy : 13.7 %\n",
      "4m 47s (- 118m 51s) (2900 3%) loss : 7.338  accuracy : 12.9 %\n",
      "4m 57s (- 118m 41s) (3000 4%) loss : 7.189  accuracy : 14.5 %\n",
      "5m 6s (- 118m 25s) (3100 4%) loss : 7.220  accuracy : 13.8 %\n",
      "5m 16s (- 118m 8s) (3200 4%) loss : 7.165  accuracy : 15.4 %\n",
      "5m 26s (- 118m 3s) (3300 4%) loss : 7.183  accuracy : 14.6 %\n",
      "5m 35s (- 117m 42s) (3400 4%) loss : 7.124  accuracy : 14.9 %\n",
      "5m 45s (- 117m 22s) (3500 4%) loss : 7.224  accuracy : 13.5 %\n",
      "5m 55s (- 117m 8s) (3600 4%) loss : 7.192  accuracy : 14.0 %\n",
      "6m 4s (- 116m 54s) (3700 4%) loss : 7.051  accuracy : 15.4 %\n",
      "6m 14s (- 116m 39s) (3800 5%) loss : 7.148  accuracy : 13.8 %\n",
      "6m 23s (- 116m 27s) (3900 5%) loss : 7.078  accuracy : 14.9 %\n",
      "6m 33s (- 116m 15s) (4000 5%) loss : 7.003  accuracy : 15.1 %\n",
      "6m 43s (- 116m 0s) (4100 5%) loss : 6.979  accuracy : 15.3 %\n",
      "6m 52s (- 115m 47s) (4200 5%) loss : 7.151  accuracy : 14.5 %\n",
      "7m 2s (- 115m 37s) (4300 5%) loss : 7.068  accuracy : 14.6 %\n",
      "7m 12s (- 115m 23s) (4400 5%) loss : 7.083  accuracy : 14.4 %\n",
      "7m 21s (- 115m 9s) (4500 6%) loss : 7.039  accuracy : 14.6 %\n",
      "7m 31s (- 115m 2s) (4600 6%) loss : 7.018  accuracy : 14.2 %\n",
      "7m 41s (- 114m 48s) (4700 6%) loss : 6.926  accuracy : 15.1 %\n",
      "7m 51s (- 114m 37s) (4800 6%) loss : 7.045  accuracy : 14.3 %\n",
      "8m 2s (- 114m 48s) (4900 6%) loss : 6.962  accuracy : 14.8 %\n",
      "8m 11s (- 114m 32s) (5000 6%) loss : 6.996  accuracy : 14.8 %\n",
      "8m 21s (- 114m 21s) (5100 6%) loss : 6.967  accuracy : 15.3 %\n",
      "8m 31s (- 114m 10s) (5200 6%) loss : 6.965  accuracy : 15.5 %\n",
      "8m 40s (- 113m 59s) (5300 7%) loss : 6.993  accuracy : 14.4 %\n",
      "8m 50s (- 113m 45s) (5400 7%) loss : 6.814  accuracy : 15.8 %\n",
      "9m 0s (- 113m 34s) (5500 7%) loss : 6.948  accuracy : 14.5 %\n",
      "9m 9s (- 113m 21s) (5600 7%) loss : 6.930  accuracy : 14.5 %\n",
      "9m 19s (- 113m 5s) (5700 7%) loss : 6.914  accuracy : 15.3 %\n",
      "9m 29s (- 112m 57s) (5800 7%) loss : 6.877  accuracy : 15.0 %\n",
      "9m 38s (- 112m 47s) (5900 7%) loss : 6.939  accuracy : 15.9 %\n",
      "9m 48s (- 112m 31s) (6000 8%) loss : 6.895  accuracy : 15.2 %\n",
      "9m 57s (- 112m 21s) (6100 8%) loss : 6.845  accuracy : 15.3 %\n",
      "10m 7s (- 112m 9s) (6200 8%) loss : 6.948  accuracy : 14.8 %\n",
      "10m 16s (- 111m 55s) (6300 8%) loss : 6.844  accuracy : 15.2 %\n",
      "10m 26s (- 111m 47s) (6400 8%) loss : 6.844  accuracy : 16.1 %\n",
      "10m 36s (- 111m 39s) (6500 8%) loss : 6.891  accuracy : 14.9 %\n",
      "10m 46s (- 111m 25s) (6600 8%) loss : 6.751  accuracy : 16.2 %\n",
      "10m 56s (- 111m 19s) (6700 8%) loss : 6.801  accuracy : 15.4 %\n",
      "11m 7s (- 111m 17s) (6800 9%) loss : 6.897  accuracy : 15.0 %\n",
      "11m 16s (- 111m 3s) (6900 9%) loss : 6.871  accuracy : 15.5 %\n",
      "11m 26s (- 110m 52s) (7000 9%) loss : 6.785  accuracy : 15.7 %\n",
      "11m 35s (- 110m 41s) (7100 9%) loss : 6.835  accuracy : 14.4 %\n",
      "11m 45s (- 110m 30s) (7200 9%) loss : 6.756  accuracy : 16.2 %\n",
      "11m 55s (- 110m 21s) (7300 9%) loss : 6.790  accuracy : 15.4 %\n",
      "12m 4s (- 110m 8s) (7400 9%) loss : 6.751  accuracy : 15.5 %\n",
      "12m 14s (- 109m 56s) (7500 10%) loss : 6.744  accuracy : 16.7 %\n",
      "12m 24s (- 109m 47s) (7600 10%) loss : 6.801  accuracy : 16.3 %\n",
      "12m 34s (- 109m 38s) (7700 10%) loss : 6.807  accuracy : 15.8 %\n",
      "12m 43s (- 109m 24s) (7800 10%) loss : 6.785  accuracy : 16.5 %\n",
      "12m 53s (- 109m 13s) (7900 10%) loss : 6.769  accuracy : 15.1 %\n",
      "13m 2s (- 109m 3s) (8000 10%) loss : 6.810  accuracy : 15.5 %\n",
      "13m 12s (- 108m 50s) (8100 10%) loss : 6.722  accuracy : 16.1 %\n",
      "13m 21s (- 108m 37s) (8200 10%) loss : 6.755  accuracy : 15.9 %\n",
      "13m 31s (- 108m 29s) (8300 11%) loss : 6.725  accuracy : 16.1 %\n",
      "13m 41s (- 108m 18s) (8400 11%) loss : 6.685  accuracy : 16.3 %\n",
      "13m 50s (- 108m 5s) (8500 11%) loss : 6.660  accuracy : 16.3 %\n",
      "14m 0s (- 107m 57s) (8600 11%) loss : 6.603  accuracy : 17.0 %\n",
      "14m 10s (- 107m 47s) (8700 11%) loss : 6.811  accuracy : 15.3 %\n",
      "14m 20s (- 107m 40s) (8800 11%) loss : 6.730  accuracy : 16.5 %\n",
      "14m 30s (- 107m 31s) (8900 11%) loss : 6.785  accuracy : 15.3 %\n",
      "14m 40s (- 107m 26s) (9000 12%) loss : 6.675  accuracy : 15.7 %\n",
      "14m 50s (- 107m 13s) (9100 12%) loss : 6.678  accuracy : 16.3 %\n",
      "15m 0s (- 107m 5s) (9200 12%) loss : 6.702  accuracy : 16.5 %\n",
      "15m 9s (- 106m 51s) (9300 12%) loss : 6.698  accuracy : 16.6 %\n",
      "15m 18s (- 106m 40s) (9400 12%) loss : 6.650  accuracy : 16.8 %\n",
      "15m 29s (- 106m 33s) (9500 12%) loss : 6.643  accuracy : 16.7 %\n",
      "15m 38s (- 106m 21s) (9600 12%) loss : 6.659  accuracy : 16.2 %\n",
      "15m 48s (- 106m 13s) (9700 12%) loss : 6.639  accuracy : 16.3 %\n",
      "15m 59s (- 106m 8s) (9800 13%) loss : 6.595  accuracy : 16.7 %\n",
      "16m 8s (- 105m 57s) (9900 13%) loss : 6.589  accuracy : 17.2 %\n",
      "16m 18s (- 105m 45s) (10000 13%) loss : 6.634  accuracy : 17.3 %\n",
      "16m 28s (- 105m 38s) (10100 13%) loss : 6.598  accuracy : 17.0 %\n",
      "16m 38s (- 105m 27s) (10200 13%) loss : 6.618  accuracy : 16.9 %\n",
      "16m 47s (- 105m 14s) (10300 13%) loss : 6.581  accuracy : 16.7 %\n",
      "16m 57s (- 105m 8s) (10400 13%) loss : 6.538  accuracy : 18.0 %\n",
      "17m 7s (- 104m 58s) (10500 14%) loss : 6.534  accuracy : 16.6 %\n",
      "17m 17s (- 104m 48s) (10600 14%) loss : 6.647  accuracy : 16.8 %\n",
      "17m 26s (- 104m 38s) (10700 14%) loss : 6.596  accuracy : 16.2 %\n",
      "17m 37s (- 104m 32s) (10800 14%) loss : 6.630  accuracy : 16.4 %\n",
      "17m 46s (- 104m 19s) (10900 14%) loss : 6.551  accuracy : 16.7 %\n",
      "17m 53s (- 103m 53s) (11000 14%) loss : 6.577  accuracy : 16.6 %\n",
      "18m 0s (- 103m 29s) (11100 14%) loss : 6.581  accuracy : 16.8 %\n",
      "18m 12s (- 103m 28s) (11200 14%) loss : 6.622  accuracy : 17.3 %\n",
      "18m 23s (- 103m 27s) (11300 15%) loss : 6.527  accuracy : 17.0 %\n",
      "18m 34s (- 103m 25s) (11400 15%) loss : 6.606  accuracy : 16.8 %\n",
      "18m 45s (- 103m 23s) (11500 15%) loss : 6.603  accuracy : 17.1 %\n",
      "18m 56s (- 103m 21s) (11600 15%) loss : 6.587  accuracy : 16.8 %\n",
      "19m 8s (- 103m 19s) (11700 15%) loss : 6.651  accuracy : 16.2 %\n",
      "19m 19s (- 103m 16s) (11800 15%) loss : 6.527  accuracy : 17.2 %\n",
      "19m 30s (- 103m 14s) (11900 15%) loss : 6.484  accuracy : 16.8 %\n",
      "19m 42s (- 103m 13s) (12000 16%) loss : 6.475  accuracy : 17.5 %\n",
      "19m 53s (- 103m 12s) (12100 16%) loss : 6.577  accuracy : 16.5 %\n",
      "20m 4s (- 103m 9s) (12200 16%) loss : 6.606  accuracy : 16.8 %\n",
      "20m 16s (- 103m 6s) (12300 16%) loss : 6.564  accuracy : 16.7 %\n",
      "20m 27s (- 103m 3s) (12400 16%) loss : 6.510  accuracy : 17.4 %\n",
      "20m 38s (- 102m 59s) (12500 16%) loss : 6.478  accuracy : 17.9 %\n",
      "20m 49s (- 102m 54s) (12600 16%) loss : 6.500  accuracy : 17.3 %\n",
      "21m 0s (- 102m 51s) (12700 16%) loss : 6.603  accuracy : 16.7 %\n",
      "21m 11s (- 102m 47s) (12800 17%) loss : 6.553  accuracy : 16.8 %\n",
      "21m 22s (- 102m 42s) (12900 17%) loss : 6.518  accuracy : 17.1 %\n",
      "21m 34s (- 102m 39s) (13000 17%) loss : 6.591  accuracy : 17.1 %\n",
      "21m 45s (- 102m 36s) (13100 17%) loss : 6.484  accuracy : 18.0 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21m 56s (- 102m 31s) (13200 17%) loss : 6.490  accuracy : 17.5 %\n",
      "22m 7s (- 102m 27s) (13300 17%) loss : 6.593  accuracy : 16.7 %\n",
      "22m 18s (- 102m 22s) (13400 17%) loss : 6.463  accuracy : 17.2 %\n",
      "22m 30s (- 102m 18s) (13500 18%) loss : 6.440  accuracy : 17.4 %\n",
      "22m 41s (- 102m 14s) (13600 18%) loss : 6.495  accuracy : 17.5 %\n",
      "22m 52s (- 102m 9s) (13700 18%) loss : 6.410  accuracy : 17.8 %\n",
      "23m 3s (- 102m 5s) (13800 18%) loss : 6.444  accuracy : 17.5 %\n",
      "23m 15s (- 101m 59s) (13900 18%) loss : 6.485  accuracy : 17.2 %\n",
      "23m 26s (- 101m 54s) (14000 18%) loss : 6.515  accuracy : 17.3 %\n",
      "23m 37s (- 101m 49s) (14100 18%) loss : 6.541  accuracy : 16.6 %\n",
      "23m 48s (- 101m 45s) (14200 18%) loss : 6.566  accuracy : 16.7 %\n",
      "24m 0s (- 101m 40s) (14300 19%) loss : 6.414  accuracy : 18.6 %\n",
      "24m 11s (- 101m 35s) (14400 19%) loss : 6.446  accuracy : 18.3 %\n",
      "24m 22s (- 101m 29s) (14500 19%) loss : 6.476  accuracy : 17.7 %\n",
      "24m 33s (- 101m 24s) (14600 19%) loss : 6.483  accuracy : 17.0 %\n",
      "24m 45s (- 101m 19s) (14700 19%) loss : 6.388  accuracy : 18.3 %\n",
      "24m 56s (- 101m 13s) (14800 19%) loss : 6.433  accuracy : 17.7 %\n",
      "25m 7s (- 101m 7s) (14900 19%) loss : 6.552  accuracy : 16.8 %\n",
      "25m 18s (- 101m 2s) (15000 20%) loss : 6.484  accuracy : 17.1 %\n",
      "25m 29s (- 100m 56s) (15100 20%) loss : 6.393  accuracy : 18.0 %\n",
      "25m 40s (- 100m 49s) (15200 20%) loss : 6.442  accuracy : 17.8 %\n",
      "25m 52s (- 100m 43s) (15300 20%) loss : 6.487  accuracy : 17.5 %\n",
      "26m 3s (- 100m 38s) (15400 20%) loss : 6.519  accuracy : 17.1 %\n",
      "26m 14s (- 100m 31s) (15500 20%) loss : 6.434  accuracy : 17.9 %\n",
      "26m 25s (- 100m 25s) (15600 20%) loss : 6.417  accuracy : 18.2 %\n",
      "26m 36s (- 100m 18s) (15700 20%) loss : 6.423  accuracy : 16.9 %\n",
      "26m 48s (- 100m 13s) (15800 21%) loss : 6.417  accuracy : 17.7 %\n",
      "26m 59s (- 100m 7s) (15900 21%) loss : 6.480  accuracy : 16.8 %\n",
      "27m 11s (- 100m 1s) (16000 21%) loss : 6.485  accuracy : 17.2 %\n",
      "27m 22s (- 99m 55s) (16100 21%) loss : 6.416  accuracy : 17.8 %\n",
      "27m 33s (- 99m 48s) (16200 21%) loss : 6.410  accuracy : 17.7 %\n",
      "27m 44s (- 99m 42s) (16300 21%) loss : 6.391  accuracy : 18.1 %\n",
      "27m 55s (- 99m 35s) (16400 21%) loss : 6.447  accuracy : 17.4 %\n",
      "28m 6s (- 99m 28s) (16500 22%) loss : 6.347  accuracy : 17.6 %\n",
      "28m 18s (- 99m 21s) (16600 22%) loss : 6.342  accuracy : 18.4 %\n",
      "28m 29s (- 99m 14s) (16700 22%) loss : 6.343  accuracy : 18.4 %\n",
      "28m 40s (- 99m 7s) (16800 22%) loss : 6.386  accuracy : 17.8 %\n",
      "28m 51s (- 99m 0s) (16900 22%) loss : 6.444  accuracy : 16.8 %\n",
      "29m 3s (- 98m 53s) (17000 22%) loss : 6.449  accuracy : 17.7 %\n",
      "29m 14s (- 98m 46s) (17100 22%) loss : 6.324  accuracy : 17.9 %\n",
      "29m 25s (- 98m 39s) (17200 22%) loss : 6.433  accuracy : 17.8 %\n",
      "29m 36s (- 98m 32s) (17300 23%) loss : 6.431  accuracy : 17.2 %\n",
      "29m 47s (- 98m 25s) (17400 23%) loss : 6.387  accuracy : 18.2 %\n",
      "29m 59s (- 98m 18s) (17500 23%) loss : 6.362  accuracy : 18.1 %\n",
      "30m 10s (- 98m 11s) (17600 23%) loss : 6.334  accuracy : 17.6 %\n",
      "30m 21s (- 98m 4s) (17700 23%) loss : 6.360  accuracy : 18.7 %\n",
      "30m 32s (- 97m 57s) (17800 23%) loss : 6.334  accuracy : 18.5 %\n",
      "30m 44s (- 97m 49s) (17900 23%) loss : 6.392  accuracy : 18.0 %\n",
      "30m 55s (- 97m 43s) (18000 24%) loss : 6.374  accuracy : 18.0 %\n",
      "31m 6s (- 97m 35s) (18100 24%) loss : 6.351  accuracy : 18.0 %\n",
      "31m 17s (- 97m 27s) (18200 24%) loss : 6.327  accuracy : 18.5 %\n",
      "31m 29s (- 97m 20s) (18300 24%) loss : 6.404  accuracy : 18.3 %\n",
      "31m 40s (- 97m 12s) (18400 24%) loss : 6.415  accuracy : 17.1 %\n",
      "31m 51s (- 97m 5s) (18500 24%) loss : 6.382  accuracy : 17.9 %\n",
      "32m 2s (- 96m 57s) (18600 24%) loss : 6.312  accuracy : 18.8 %\n",
      "32m 14s (- 96m 50s) (18700 24%) loss : 6.387  accuracy : 17.3 %\n",
      "32m 25s (- 96m 42s) (18800 25%) loss : 6.317  accuracy : 17.7 %\n",
      "32m 36s (- 96m 35s) (18900 25%) loss : 6.425  accuracy : 17.6 %\n",
      "32m 48s (- 96m 27s) (19000 25%) loss : 6.405  accuracy : 17.5 %\n",
      "32m 59s (- 96m 19s) (19100 25%) loss : 6.440  accuracy : 17.4 %\n",
      "33m 10s (- 96m 11s) (19200 25%) loss : 6.393  accuracy : 17.8 %\n",
      "33m 21s (- 96m 3s) (19300 25%) loss : 6.372  accuracy : 18.3 %\n",
      "33m 32s (- 95m 55s) (19400 25%) loss : 6.330  accuracy : 18.5 %\n",
      "33m 43s (- 95m 47s) (19500 26%) loss : 6.378  accuracy : 17.8 %\n",
      "33m 55s (- 95m 39s) (19600 26%) loss : 6.322  accuracy : 17.6 %\n",
      "34m 6s (- 95m 31s) (19700 26%) loss : 6.252  accuracy : 18.4 %\n",
      "34m 17s (- 95m 23s) (19800 26%) loss : 6.285  accuracy : 18.8 %\n",
      "34m 28s (- 95m 15s) (19900 26%) loss : 6.321  accuracy : 18.5 %\n",
      "34m 40s (- 95m 8s) (20000 26%) loss : 6.327  accuracy : 18.0 %\n",
      "34m 52s (- 95m 1s) (20100 26%) loss : 6.280  accuracy : 18.4 %\n",
      "35m 3s (- 94m 52s) (20200 26%) loss : 6.317  accuracy : 17.8 %\n",
      "35m 14s (- 94m 44s) (20300 27%) loss : 6.305  accuracy : 18.1 %\n",
      "35m 25s (- 94m 36s) (20400 27%) loss : 6.247  accuracy : 18.7 %\n",
      "35m 37s (- 94m 28s) (20500 27%) loss : 6.388  accuracy : 17.5 %\n",
      "35m 48s (- 94m 20s) (20600 27%) loss : 6.349  accuracy : 18.1 %\n",
      "35m 59s (- 94m 12s) (20700 27%) loss : 6.373  accuracy : 19.0 %\n",
      "36m 10s (- 94m 3s) (20800 27%) loss : 6.269  accuracy : 19.0 %\n",
      "36m 17s (- 93m 42s) (20900 27%) loss : 6.323  accuracy : 17.4 %\n",
      "36m 24s (- 93m 23s) (21000 28%) loss : 6.306  accuracy : 18.8 %\n",
      "36m 30s (- 93m 2s) (21100 28%) loss : 6.333  accuracy : 18.4 %\n",
      "36m 37s (- 92m 43s) (21200 28%) loss : 6.361  accuracy : 17.2 %\n",
      "36m 45s (- 92m 28s) (21300 28%) loss : 6.330  accuracy : 17.5 %\n",
      "36m 54s (- 92m 13s) (21400 28%) loss : 6.332  accuracy : 18.4 %\n",
      "37m 0s (- 91m 52s) (21500 28%) loss : 6.282  accuracy : 18.0 %\n",
      "37m 7s (- 91m 32s) (21600 28%) loss : 6.245  accuracy : 18.8 %\n",
      "37m 13s (- 91m 14s) (21700 28%) loss : 6.271  accuracy : 18.8 %\n",
      "37m 20s (- 90m 54s) (21800 29%) loss : 6.347  accuracy : 17.9 %\n",
      "37m 26s (- 90m 34s) (21900 29%) loss : 6.300  accuracy : 18.5 %\n",
      "37m 32s (- 90m 13s) (22000 29%) loss : 6.304  accuracy : 18.2 %\n",
      "37m 38s (- 89m 53s) (22100 29%) loss : 6.340  accuracy : 18.0 %\n",
      "37m 45s (- 89m 34s) (22200 29%) loss : 6.326  accuracy : 18.4 %\n",
      "37m 51s (- 89m 15s) (22300 29%) loss : 6.270  accuracy : 19.4 %\n",
      "37m 58s (- 88m 56s) (22400 29%) loss : 6.243  accuracy : 18.7 %\n",
      "38m 4s (- 88m 37s) (22500 30%) loss : 6.239  accuracy : 18.5 %\n",
      "38m 11s (- 88m 19s) (22600 30%) loss : 6.243  accuracy : 18.5 %\n",
      "38m 21s (- 88m 8s) (22700 30%) loss : 6.257  accuracy : 18.4 %\n",
      "38m 33s (- 88m 3s) (22800 30%) loss : 6.287  accuracy : 18.4 %\n",
      "38m 45s (- 87m 57s) (22900 30%) loss : 6.245  accuracy : 19.0 %\n",
      "38m 56s (- 87m 50s) (23000 30%) loss : 6.303  accuracy : 17.9 %\n",
      "39m 8s (- 87m 44s) (23100 30%) loss : 6.261  accuracy : 18.2 %\n",
      "39m 20s (- 87m 38s) (23200 30%) loss : 6.198  accuracy : 18.7 %\n",
      "39m 32s (- 87m 31s) (23300 31%) loss : 6.194  accuracy : 19.0 %\n",
      "39m 44s (- 87m 25s) (23400 31%) loss : 6.257  accuracy : 17.6 %\n",
      "39m 56s (- 87m 19s) (23500 31%) loss : 6.274  accuracy : 18.2 %\n",
      "40m 8s (- 87m 12s) (23600 31%) loss : 6.302  accuracy : 17.3 %\n",
      "40m 20s (- 87m 6s) (23700 31%) loss : 6.232  accuracy : 17.7 %\n",
      "40m 32s (- 86m 59s) (23800 31%) loss : 6.314  accuracy : 18.2 %\n",
      "40m 43s (- 86m 52s) (23900 31%) loss : 6.277  accuracy : 18.8 %\n",
      "40m 55s (- 86m 45s) (24000 32%) loss : 6.266  accuracy : 18.8 %\n",
      "41m 7s (- 86m 39s) (24100 32%) loss : 6.285  accuracy : 18.5 %\n",
      "41m 20s (- 86m 33s) (24200 32%) loss : 6.243  accuracy : 18.5 %\n",
      "41m 32s (- 86m 26s) (24300 32%) loss : 6.252  accuracy : 18.0 %\n",
      "41m 43s (- 86m 19s) (24400 32%) loss : 6.325  accuracy : 18.3 %\n",
      "41m 55s (- 86m 12s) (24500 32%) loss : 6.244  accuracy : 18.5 %\n",
      "42m 7s (- 86m 5s) (24600 32%) loss : 6.260  accuracy : 18.5 %\n",
      "42m 19s (- 85m 57s) (24700 32%) loss : 6.214  accuracy : 18.2 %\n",
      "42m 30s (- 85m 50s) (24800 33%) loss : 6.250  accuracy : 19.2 %\n",
      "42m 42s (- 85m 43s) (24900 33%) loss : 6.181  accuracy : 18.7 %\n",
      "42m 54s (- 85m 36s) (25000 33%) loss : 6.328  accuracy : 18.2 %\n",
      "43m 7s (- 85m 30s) (25100 33%) loss : 6.247  accuracy : 18.2 %\n",
      "43m 19s (- 85m 23s) (25200 33%) loss : 6.175  accuracy : 19.8 %\n",
      "43m 30s (- 85m 15s) (25300 33%) loss : 6.151  accuracy : 18.8 %\n",
      "43m 42s (- 85m 8s) (25400 33%) loss : 6.162  accuracy : 19.4 %\n",
      "43m 54s (- 85m 0s) (25500 34%) loss : 6.257  accuracy : 18.1 %\n",
      "44m 6s (- 84m 53s) (25600 34%) loss : 6.171  accuracy : 19.8 %\n",
      "44m 18s (- 84m 45s) (25700 34%) loss : 6.235  accuracy : 18.6 %\n",
      "44m 29s (- 84m 38s) (25800 34%) loss : 6.214  accuracy : 19.1 %\n",
      "44m 41s (- 84m 30s) (25900 34%) loss : 6.186  accuracy : 18.8 %\n",
      "44m 53s (- 84m 22s) (26000 34%) loss : 6.164  accuracy : 19.0 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45m 5s (- 84m 15s) (26100 34%) loss : 6.268  accuracy : 18.3 %\n",
      "45m 16s (- 84m 7s) (26200 34%) loss : 6.171  accuracy : 19.4 %\n",
      "45m 28s (- 83m 59s) (26300 35%) loss : 6.281  accuracy : 18.4 %\n",
      "45m 40s (- 83m 51s) (26400 35%) loss : 6.193  accuracy : 19.1 %\n",
      "45m 52s (- 83m 44s) (26500 35%) loss : 6.205  accuracy : 19.0 %\n",
      "46m 4s (- 83m 37s) (26600 35%) loss : 6.215  accuracy : 19.2 %\n",
      "46m 16s (- 83m 29s) (26700 35%) loss : 6.247  accuracy : 19.2 %\n",
      "46m 28s (- 83m 22s) (26800 35%) loss : 6.227  accuracy : 18.8 %\n",
      "46m 41s (- 83m 15s) (26900 35%) loss : 6.240  accuracy : 18.3 %\n",
      "46m 53s (- 83m 8s) (27000 36%) loss : 6.236  accuracy : 18.5 %\n",
      "47m 5s (- 83m 1s) (27100 36%) loss : 6.162  accuracy : 20.2 %\n",
      "47m 18s (- 82m 54s) (27200 36%) loss : 6.182  accuracy : 19.2 %\n",
      "47m 29s (- 82m 46s) (27300 36%) loss : 6.189  accuracy : 18.7 %\n",
      "47m 41s (- 82m 38s) (27400 36%) loss : 6.210  accuracy : 19.2 %\n",
      "47m 53s (- 82m 30s) (27500 36%) loss : 6.218  accuracy : 18.4 %\n",
      "48m 5s (- 82m 21s) (27600 36%) loss : 6.252  accuracy : 18.6 %\n",
      "48m 16s (- 82m 13s) (27700 36%) loss : 6.078  accuracy : 19.0 %\n",
      "48m 28s (- 82m 4s) (27800 37%) loss : 6.281  accuracy : 18.9 %\n",
      "48m 40s (- 81m 56s) (27900 37%) loss : 6.105  accuracy : 19.3 %\n",
      "48m 51s (- 81m 47s) (28000 37%) loss : 6.166  accuracy : 19.8 %\n",
      "49m 3s (- 81m 39s) (28100 37%) loss : 6.189  accuracy : 19.0 %\n",
      "49m 15s (- 81m 30s) (28200 37%) loss : 6.142  accuracy : 19.0 %\n",
      "49m 26s (- 81m 22s) (28300 37%) loss : 6.168  accuracy : 19.0 %\n",
      "49m 38s (- 81m 13s) (28400 37%) loss : 6.174  accuracy : 19.6 %\n",
      "49m 49s (- 81m 5s) (28500 38%) loss : 6.146  accuracy : 18.2 %\n",
      "50m 1s (- 80m 56s) (28600 38%) loss : 6.148  accuracy : 20.0 %\n",
      "50m 13s (- 80m 47s) (28700 38%) loss : 6.251  accuracy : 18.1 %\n",
      "50m 24s (- 80m 39s) (28800 38%) loss : 6.106  accuracy : 19.6 %\n",
      "50m 36s (- 80m 30s) (28900 38%) loss : 6.163  accuracy : 19.3 %\n",
      "50m 48s (- 80m 21s) (29000 38%) loss : 6.141  accuracy : 19.1 %\n",
      "51m 16s (- 80m 39s) (29100 38%) loss : 6.203  accuracy : 19.0 %\n",
      "51m 28s (- 80m 31s) (29200 38%) loss : 6.220  accuracy : 18.7 %\n",
      "51m 40s (- 80m 22s) (29300 39%) loss : 6.100  accuracy : 19.8 %\n",
      "51m 52s (- 80m 13s) (29400 39%) loss : 6.192  accuracy : 18.5 %\n",
      "52m 4s (- 80m 5s) (29500 39%) loss : 6.172  accuracy : 18.9 %\n",
      "52m 15s (- 79m 56s) (29600 39%) loss : 6.207  accuracy : 18.9 %\n",
      "52m 27s (- 79m 47s) (29700 39%) loss : 6.175  accuracy : 19.3 %\n",
      "52m 38s (- 79m 38s) (29800 39%) loss : 6.156  accuracy : 19.1 %\n",
      "52m 50s (- 79m 29s) (29900 39%) loss : 6.073  accuracy : 19.6 %\n",
      "53m 2s (- 79m 20s) (30000 40%) loss : 6.089  accuracy : 19.6 %\n",
      "53m 14s (- 79m 11s) (30100 40%) loss : 6.173  accuracy : 19.1 %\n",
      "53m 25s (- 79m 1s) (30200 40%) loss : 6.097  accuracy : 19.5 %\n",
      "53m 37s (- 78m 52s) (30300 40%) loss : 6.120  accuracy : 19.8 %\n",
      "53m 48s (- 78m 43s) (30400 40%) loss : 6.218  accuracy : 19.4 %\n",
      "54m 0s (- 78m 34s) (30500 40%) loss : 6.078  accuracy : 19.6 %\n",
      "54m 12s (- 78m 25s) (30600 40%) loss : 6.114  accuracy : 18.8 %\n",
      "54m 23s (- 78m 16s) (30700 41%) loss : 6.111  accuracy : 19.5 %\n",
      "54m 35s (- 78m 6s) (30800 41%) loss : 6.088  accuracy : 20.0 %\n",
      "54m 46s (- 77m 57s) (30900 41%) loss : 6.211  accuracy : 19.2 %\n",
      "54m 58s (- 77m 48s) (31000 41%) loss : 6.197  accuracy : 18.8 %\n",
      "55m 9s (- 77m 38s) (31100 41%) loss : 6.090  accuracy : 19.8 %\n",
      "55m 21s (- 77m 29s) (31200 41%) loss : 6.062  accuracy : 19.7 %\n",
      "55m 33s (- 77m 20s) (31300 41%) loss : 6.080  accuracy : 19.4 %\n",
      "55m 44s (- 77m 11s) (31400 41%) loss : 6.186  accuracy : 19.1 %\n",
      "55m 56s (- 77m 1s) (31500 42%) loss : 6.122  accuracy : 18.9 %\n",
      "56m 8s (- 76m 52s) (31600 42%) loss : 6.133  accuracy : 19.5 %\n",
      "56m 19s (- 76m 43s) (31700 42%) loss : 6.136  accuracy : 19.3 %\n",
      "56m 31s (- 76m 33s) (31800 42%) loss : 6.084  accuracy : 18.8 %\n",
      "56m 43s (- 76m 25s) (31900 42%) loss : 6.187  accuracy : 19.4 %\n",
      "56m 55s (- 76m 15s) (32000 42%) loss : 6.054  accuracy : 20.0 %\n",
      "57m 6s (- 76m 6s) (32100 42%) loss : 6.089  accuracy : 20.0 %\n",
      "57m 18s (- 75m 56s) (32200 43%) loss : 6.134  accuracy : 19.0 %\n",
      "57m 30s (- 75m 47s) (32300 43%) loss : 6.082  accuracy : 19.9 %\n",
      "57m 41s (- 75m 38s) (32400 43%) loss : 6.130  accuracy : 19.4 %\n",
      "57m 53s (- 75m 28s) (32500 43%) loss : 6.066  accuracy : 20.2 %\n",
      "58m 4s (- 75m 19s) (32600 43%) loss : 6.134  accuracy : 19.0 %\n",
      "58m 16s (- 75m 9s) (32700 43%) loss : 6.120  accuracy : 19.4 %\n",
      "58m 28s (- 75m 0s) (32800 43%) loss : 6.142  accuracy : 19.5 %\n",
      "58m 40s (- 74m 51s) (32900 43%) loss : 6.125  accuracy : 19.7 %\n",
      "58m 51s (- 74m 41s) (33000 44%) loss : 6.141  accuracy : 18.9 %\n",
      "59m 3s (- 74m 32s) (33100 44%) loss : 6.121  accuracy : 19.6 %\n",
      "59m 15s (- 74m 22s) (33200 44%) loss : 6.137  accuracy : 19.2 %\n",
      "59m 26s (- 74m 13s) (33300 44%) loss : 6.092  accuracy : 19.6 %\n",
      "59m 38s (- 74m 3s) (33400 44%) loss : 6.162  accuracy : 19.2 %\n",
      "59m 50s (- 73m 54s) (33500 44%) loss : 6.081  accuracy : 20.0 %\n",
      "60m 1s (- 73m 44s) (33600 44%) loss : 6.017  accuracy : 21.0 %\n",
      "60m 13s (- 73m 35s) (33700 45%) loss : 6.202  accuracy : 19.0 %\n",
      "60m 25s (- 73m 25s) (33800 45%) loss : 6.117  accuracy : 19.9 %\n",
      "60m 36s (- 73m 15s) (33900 45%) loss : 6.087  accuracy : 19.2 %\n",
      "60m 48s (- 73m 6s) (34000 45%) loss : 6.139  accuracy : 20.1 %\n",
      "60m 57s (- 72m 53s) (34100 45%) loss : 6.118  accuracy : 19.5 %\n",
      "61m 3s (- 72m 37s) (34200 45%) loss : 6.094  accuracy : 19.4 %\n",
      "61m 9s (- 72m 20s) (34300 45%) loss : 6.018  accuracy : 19.9 %\n",
      "61m 15s (- 72m 4s) (34400 45%) loss : 5.987  accuracy : 19.9 %\n",
      "61m 21s (- 71m 48s) (34500 46%) loss : 6.080  accuracy : 19.3 %\n",
      "61m 27s (- 71m 32s) (34600 46%) loss : 6.075  accuracy : 19.9 %\n",
      "61m 33s (- 71m 16s) (34700 46%) loss : 6.079  accuracy : 19.4 %\n",
      "61m 39s (- 71m 0s) (34800 46%) loss : 6.171  accuracy : 18.4 %\n",
      "61m 45s (- 70m 44s) (34900 46%) loss : 6.158  accuracy : 18.8 %\n",
      "61m 51s (- 70m 28s) (35000 46%) loss : 6.083  accuracy : 19.0 %\n",
      "61m 57s (- 70m 12s) (35100 46%) loss : 6.073  accuracy : 19.5 %\n",
      "62m 3s (- 69m 56s) (35200 47%) loss : 6.112  accuracy : 20.0 %\n",
      "62m 9s (- 69m 41s) (35300 47%) loss : 6.140  accuracy : 18.7 %\n",
      "62m 15s (- 69m 25s) (35400 47%) loss : 6.141  accuracy : 19.1 %\n",
      "62m 22s (- 69m 11s) (35500 47%) loss : 6.077  accuracy : 19.4 %\n",
      "62m 31s (- 68m 58s) (35600 47%) loss : 6.050  accuracy : 19.7 %\n",
      "62m 38s (- 68m 43s) (35700 47%) loss : 6.055  accuracy : 20.0 %\n",
      "62m 44s (- 68m 28s) (35800 47%) loss : 6.130  accuracy : 19.2 %\n",
      "62m 50s (- 68m 13s) (35900 47%) loss : 6.059  accuracy : 19.6 %\n",
      "62m 56s (- 67m 58s) (36000 48%) loss : 6.108  accuracy : 19.3 %\n",
      "63m 2s (- 67m 42s) (36100 48%) loss : 6.082  accuracy : 19.7 %\n",
      "63m 8s (- 67m 27s) (36200 48%) loss : 6.086  accuracy : 19.5 %\n",
      "63m 14s (- 67m 12s) (36300 48%) loss : 6.081  accuracy : 19.9 %\n",
      "63m 20s (- 66m 57s) (36400 48%) loss : 6.119  accuracy : 19.0 %\n",
      "63m 26s (- 66m 41s) (36500 48%) loss : 6.056  accuracy : 19.9 %\n",
      "63m 32s (- 66m 26s) (36600 48%) loss : 6.095  accuracy : 19.4 %\n",
      "63m 38s (- 66m 11s) (36700 49%) loss : 6.078  accuracy : 20.2 %\n",
      "63m 44s (- 65m 56s) (36800 49%) loss : 6.041  accuracy : 19.2 %\n",
      "63m 50s (- 65m 42s) (36900 49%) loss : 6.156  accuracy : 18.6 %\n",
      "63m 56s (- 65m 27s) (37000 49%) loss : 6.076  accuracy : 19.0 %\n",
      "64m 4s (- 65m 14s) (37100 49%) loss : 6.091  accuracy : 19.0 %\n",
      "64m 15s (- 65m 4s) (37200 49%) loss : 6.011  accuracy : 20.5 %\n",
      "64m 25s (- 64m 54s) (37300 49%) loss : 6.100  accuracy : 20.4 %\n",
      "64m 36s (- 64m 43s) (37400 49%) loss : 6.054  accuracy : 20.0 %\n",
      "64m 46s (- 64m 33s) (37500 50%) loss : 6.056  accuracy : 19.4 %\n",
      "64m 56s (- 64m 23s) (37600 50%) loss : 6.032  accuracy : 19.9 %\n",
      "65m 7s (- 64m 12s) (37700 50%) loss : 6.032  accuracy : 20.0 %\n",
      "65m 17s (- 64m 2s) (37800 50%) loss : 6.027  accuracy : 20.0 %\n",
      "65m 27s (- 63m 51s) (37900 50%) loss : 6.089  accuracy : 19.0 %\n",
      "65m 55s (- 63m 58s) (38000 50%) loss : 5.988  accuracy : 20.0 %\n",
      "66m 5s (- 63m 47s) (38100 50%) loss : 6.022  accuracy : 19.7 %\n",
      "66m 15s (- 63m 36s) (38200 51%) loss : 6.026  accuracy : 20.5 %\n",
      "66m 25s (- 63m 26s) (38300 51%) loss : 6.056  accuracy : 19.9 %\n",
      "66m 36s (- 63m 15s) (38400 51%) loss : 6.087  accuracy : 19.2 %\n",
      "66m 46s (- 63m 5s) (38500 51%) loss : 6.090  accuracy : 19.4 %\n",
      "66m 56s (- 62m 54s) (38600 51%) loss : 6.077  accuracy : 20.0 %\n",
      "67m 7s (- 62m 44s) (38700 51%) loss : 6.097  accuracy : 19.8 %\n",
      "67m 17s (- 62m 33s) (38800 51%) loss : 6.040  accuracy : 19.9 %\n",
      "67m 27s (- 62m 23s) (38900 51%) loss : 6.000  accuracy : 19.5 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67m 37s (- 62m 12s) (39000 52%) loss : 5.919  accuracy : 20.7 %\n",
      "67m 48s (- 62m 2s) (39100 52%) loss : 6.076  accuracy : 19.8 %\n",
      "67m 58s (- 61m 51s) (39200 52%) loss : 6.039  accuracy : 19.2 %\n",
      "68m 8s (- 61m 41s) (39300 52%) loss : 5.958  accuracy : 20.5 %\n",
      "68m 18s (- 61m 30s) (39400 52%) loss : 6.086  accuracy : 18.7 %\n",
      "68m 29s (- 61m 19s) (39500 52%) loss : 6.092  accuracy : 19.6 %\n",
      "68m 39s (- 61m 9s) (39600 52%) loss : 5.966  accuracy : 20.2 %\n",
      "68m 57s (- 61m 5s) (39700 53%) loss : 5.992  accuracy : 20.6 %\n",
      "69m 17s (- 61m 3s) (39800 53%) loss : 6.070  accuracy : 20.2 %\n",
      "69m 27s (- 60m 52s) (39900 53%) loss : 5.957  accuracy : 20.7 %\n",
      "69m 37s (- 60m 42s) (40000 53%) loss : 6.034  accuracy : 19.5 %\n",
      "69m 47s (- 60m 31s) (40100 53%) loss : 6.064  accuracy : 19.7 %\n",
      "69m 57s (- 60m 20s) (40200 53%) loss : 6.002  accuracy : 20.4 %\n",
      "70m 8s (- 60m 10s) (40300 53%) loss : 6.112  accuracy : 19.2 %\n",
      "70m 18s (- 59m 59s) (40400 53%) loss : 6.060  accuracy : 20.3 %\n",
      "70m 28s (- 59m 48s) (40500 54%) loss : 6.018  accuracy : 19.5 %\n",
      "70m 38s (- 59m 38s) (40600 54%) loss : 6.040  accuracy : 19.1 %\n",
      "70m 49s (- 59m 27s) (40700 54%) loss : 6.073  accuracy : 19.0 %\n",
      "70m 59s (- 59m 17s) (40800 54%) loss : 5.988  accuracy : 19.8 %\n",
      "71m 9s (- 59m 6s) (40900 54%) loss : 6.057  accuracy : 19.3 %\n",
      "71m 20s (- 58m 56s) (41000 54%) loss : 6.055  accuracy : 18.9 %\n",
      "71m 31s (- 58m 46s) (41100 54%) loss : 6.004  accuracy : 19.7 %\n",
      "71m 53s (- 58m 45s) (41200 55%) loss : 6.034  accuracy : 19.8 %\n",
      "72m 8s (- 58m 38s) (41300 55%) loss : 6.097  accuracy : 18.5 %\n",
      "72m 18s (- 58m 28s) (41400 55%) loss : 6.000  accuracy : 20.6 %\n",
      "72m 29s (- 58m 17s) (41500 55%) loss : 6.050  accuracy : 20.2 %\n",
      "72m 39s (- 58m 6s) (41600 55%) loss : 6.044  accuracy : 19.8 %\n",
      "72m 49s (- 57m 56s) (41700 55%) loss : 5.997  accuracy : 20.0 %\n",
      "72m 59s (- 57m 45s) (41800 55%) loss : 6.001  accuracy : 19.6 %\n",
      "73m 10s (- 57m 34s) (41900 55%) loss : 6.011  accuracy : 20.4 %\n",
      "73m 20s (- 57m 24s) (42000 56%) loss : 5.962  accuracy : 20.0 %\n",
      "73m 30s (- 57m 13s) (42100 56%) loss : 6.056  accuracy : 19.8 %\n",
      "73m 40s (- 57m 3s) (42200 56%) loss : 6.055  accuracy : 19.6 %\n",
      "73m 51s (- 56m 52s) (42300 56%) loss : 6.001  accuracy : 20.0 %\n",
      "74m 1s (- 56m 41s) (42400 56%) loss : 5.919  accuracy : 20.8 %\n",
      "74m 11s (- 56m 31s) (42500 56%) loss : 5.991  accuracy : 19.9 %\n",
      "74m 21s (- 56m 20s) (42600 56%) loss : 6.037  accuracy : 19.6 %\n",
      "74m 37s (- 56m 13s) (42700 57%) loss : 6.083  accuracy : 19.8 %\n",
      "74m 59s (- 56m 12s) (42800 57%) loss : 6.066  accuracy : 19.7 %\n",
      "75m 9s (- 56m 1s) (42900 57%) loss : 6.027  accuracy : 19.8 %\n",
      "75m 20s (- 55m 50s) (43000 57%) loss : 5.970  accuracy : 20.4 %\n",
      "75m 30s (- 55m 39s) (43100 57%) loss : 5.933  accuracy : 19.8 %\n",
      "75m 40s (- 55m 29s) (43200 57%) loss : 6.033  accuracy : 20.0 %\n",
      "76m 7s (- 55m 30s) (43300 57%) loss : 5.998  accuracy : 19.7 %\n",
      "76m 18s (- 55m 20s) (43400 57%) loss : 5.999  accuracy : 19.4 %\n",
      "76m 28s (- 55m 9s) (43500 58%) loss : 5.924  accuracy : 20.5 %\n",
      "76m 38s (- 54m 58s) (43600 58%) loss : 6.056  accuracy : 19.1 %\n",
      "76m 49s (- 54m 47s) (43700 58%) loss : 5.998  accuracy : 20.5 %\n",
      "76m 59s (- 54m 37s) (43800 58%) loss : 5.979  accuracy : 20.0 %\n",
      "77m 9s (- 54m 26s) (43900 58%) loss : 6.021  accuracy : 20.1 %\n",
      "77m 19s (- 54m 15s) (44000 58%) loss : 5.988  accuracy : 20.0 %\n",
      "77m 30s (- 54m 5s) (44100 58%) loss : 5.964  accuracy : 20.7 %\n",
      "77m 40s (- 53m 54s) (44200 59%) loss : 5.933  accuracy : 20.7 %\n",
      "77m 50s (- 53m 43s) (44300 59%) loss : 6.023  accuracy : 20.5 %\n",
      "78m 0s (- 53m 32s) (44400 59%) loss : 5.913  accuracy : 20.7 %\n",
      "78m 10s (- 53m 21s) (44500 59%) loss : 5.960  accuracy : 20.1 %\n",
      "78m 21s (- 53m 11s) (44600 59%) loss : 5.985  accuracy : 20.2 %\n",
      "78m 31s (- 53m 0s) (44700 59%) loss : 5.964  accuracy : 20.9 %\n",
      "78m 41s (- 52m 49s) (44800 59%) loss : 6.019  accuracy : 19.6 %\n",
      "78m 52s (- 52m 39s) (44900 59%) loss : 5.907  accuracy : 21.2 %\n",
      "79m 2s (- 52m 28s) (45000 60%) loss : 6.056  accuracy : 19.6 %\n",
      "79m 13s (- 52m 17s) (45100 60%) loss : 5.990  accuracy : 20.2 %\n",
      "79m 23s (- 52m 7s) (45200 60%) loss : 5.937  accuracy : 20.4 %\n",
      "79m 33s (- 51m 56s) (45300 60%) loss : 6.056  accuracy : 18.6 %\n",
      "79m 44s (- 51m 45s) (45400 60%) loss : 6.024  accuracy : 19.7 %\n",
      "79m 54s (- 51m 35s) (45500 60%) loss : 6.020  accuracy : 19.9 %\n",
      "80m 4s (- 51m 24s) (45600 60%) loss : 5.984  accuracy : 20.4 %\n",
      "80m 14s (- 51m 13s) (45700 61%) loss : 5.924  accuracy : 20.6 %\n",
      "80m 25s (- 51m 2s) (45800 61%) loss : 5.989  accuracy : 19.3 %\n",
      "80m 35s (- 50m 52s) (45900 61%) loss : 5.940  accuracy : 20.1 %\n",
      "80m 45s (- 50m 41s) (46000 61%) loss : 5.985  accuracy : 20.3 %\n",
      "80m 56s (- 50m 30s) (46100 61%) loss : 5.958  accuracy : 20.1 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-c4649f694b9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcbow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNgrams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0005\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-41-ab4452af9994>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, ngrams, iters, epochs, lr, random_state, print_every, compute_accuracy)\u001b[0m\n\u001b[0;32m    140\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mcouple\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mngrams\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m                     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainLoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcouple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m                     \u001b[0mtot_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                     \u001b[0mtot_loss_words\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-41-ab4452af9994>\u001b[0m in \u001b[0;36mtrainLoop\u001b[1;34m(couple, optimizer, compute_accuracy)\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m             \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomputeAccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcompute_accuracy\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-41-ab4452af9994>\u001b[0m in \u001b[0;36mcomputeAccuracy\u001b[1;34m(log_probs, targets)\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m             acc = sum([log_probs[i, :, j].data.topk(1)[1].item() == targets[i, j].item() \n\u001b[1;32m---> 98\u001b[1;33m                        \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m                        for j in range(targets.size(1))])\n\u001b[0;32m    100\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-41-ab4452af9994>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     97\u001b[0m             acc = sum([log_probs[i, :, j].data.topk(1)[1].item() == targets[i, j].item() \n\u001b[0;32m     98\u001b[0m                        \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m                        for j in range(targets.size(1))])\n\u001b[0m\u001b[0;32m    100\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cbow.train(Ngrams, epochs = 1, lr = 0.0005, print_every = 100, compute_accuracy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.computeSimilarities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('johan', 0.39462912),\n",
       " ('definition', 0.3945577),\n",
       " ('neighbourhood', 0.35367444),\n",
       " ('tonight', 0.34645903),\n",
       " ('mocking', 0.3464274),\n",
       " ('discoverer', 0.34625724),\n",
       " ('enact', 0.34261838),\n",
       " ('organized', 0.34073338),\n",
       " ('screens', 0.339069),\n",
       " ('memo', 0.33459648)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar(word = 'car', bound = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save & Load<br>\n",
    "\n",
    "The lightweight word2vec model can be saved for further use, or alternatively the full shell wrapping the word2vec model can be saved for subsequent training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "#torch.save(word2vec, path_to_DL4NLP + '\\\\saves\\\\DL4NLP_I1_cbow.pt')\n",
    "\n",
    "# load\n",
    "#word2vec = torch.load(path_to_DL4NLP + '\\\\saves\\\\DL4NLP_I1_cbow.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Training with SkipGram objective\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "\n",
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = Lang(corpus_tokenized, base_tokens = ['SOS', 'EOS', 'UNK'], min_count = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipgram.word2vec = word2vec : True\n"
     ]
    }
   ],
   "source": [
    "word2vec = myWord2Vec(lang, T = 100)\n",
    "skipgram = Word2VecShell(word2vec, device, sg = 1, context_size = 5)\n",
    "print('skipgram.word2vec = word2vec :', skipgram.word2vec == word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19435"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ngrams = skipgram.generatePackedNgrams(corpus_tokenized, context_size = 5, batch_size = 64, seed = 42)\n",
    "len(Ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "0m 39s (- 127m 41s) (100 0%) loss : 14.977  accuracy : 0.6 %\n",
      "1m 19s (- 127m 1s) (200 1%) loss : 10.444  accuracy : 1.9 %\n",
      "1m 57s (- 124m 44s) (300 1%) loss : 9.782  accuracy : 2.7 %\n",
      "2m 34s (- 122m 41s) (400 2%) loss : 9.573  accuracy : 3.4 %\n",
      "3m 12s (- 121m 19s) (500 2%) loss : 9.494  accuracy : 3.7 %\n",
      "3m 49s (- 120m 0s) (600 3%) loss : 9.405  accuracy : 3.7 %\n",
      "4m 26s (- 118m 56s) (700 3%) loss : 9.318  accuracy : 4.0 %\n",
      "5m 4s (- 118m 2s) (800 4%) loss : 9.236  accuracy : 4.1 %\n",
      "5m 41s (- 117m 12s) (900 4%) loss : 9.177  accuracy : 4.1 %\n",
      "6m 18s (- 116m 15s) (1000 5%) loss : 9.114  accuracy : 4.0 %\n",
      "6m 55s (- 115m 18s) (1100 5%) loss : 9.034  accuracy : 4.0 %\n",
      "7m 31s (- 114m 27s) (1200 6%) loss : 8.985  accuracy : 4.0 %\n",
      "8m 9s (- 113m 41s) (1300 6%) loss : 8.963  accuracy : 3.9 %\n",
      "8m 46s (- 112m 58s) (1400 7%) loss : 8.892  accuracy : 3.9 %\n",
      "9m 23s (- 112m 12s) (1500 7%) loss : 8.847  accuracy : 4.1 %\n",
      "9m 59s (- 111m 26s) (1600 8%) loss : 8.804  accuracy : 4.0 %\n",
      "10m 36s (- 110m 38s) (1700 8%) loss : 8.736  accuracy : 4.2 %\n",
      "11m 13s (- 109m 57s) (1800 9%) loss : 8.752  accuracy : 3.8 %\n",
      "11m 50s (- 109m 15s) (1900 9%) loss : 8.694  accuracy : 3.8 %\n",
      "12m 27s (- 108m 36s) (2000 10%) loss : 8.682  accuracy : 3.9 %\n",
      "13m 4s (- 107m 55s) (2100 10%) loss : 8.636  accuracy : 4.2 %\n",
      "13m 41s (- 107m 15s) (2200 11%) loss : 8.628  accuracy : 3.9 %\n",
      "14m 18s (- 106m 34s) (2300 11%) loss : 8.585  accuracy : 3.9 %\n",
      "14m 55s (- 105m 55s) (2400 12%) loss : 8.539  accuracy : 4.1 %\n",
      "15m 32s (- 105m 14s) (2500 12%) loss : 8.517  accuracy : 4.0 %\n",
      "16m 9s (- 104m 34s) (2600 13%) loss : 8.504  accuracy : 4.0 %\n",
      "16m 45s (- 103m 55s) (2700 13%) loss : 8.497  accuracy : 3.9 %\n",
      "17m 22s (- 103m 14s) (2800 14%) loss : 8.473  accuracy : 4.1 %\n",
      "17m 59s (- 102m 35s) (2900 14%) loss : 8.461  accuracy : 4.0 %\n",
      "18m 36s (- 101m 56s) (3000 15%) loss : 8.462  accuracy : 3.9 %\n",
      "19m 13s (- 101m 17s) (3100 15%) loss : 8.396  accuracy : 3.9 %\n",
      "19m 50s (- 100m 37s) (3200 16%) loss : 8.413  accuracy : 4.0 %\n",
      "20m 26s (- 99m 58s) (3300 16%) loss : 8.389  accuracy : 3.9 %\n",
      "21m 3s (- 99m 18s) (3400 17%) loss : 8.366  accuracy : 4.1 %\n",
      "21m 40s (- 98m 40s) (3500 18%) loss : 8.344  accuracy : 4.1 %\n",
      "22m 17s (- 98m 1s) (3600 18%) loss : 8.338  accuracy : 4.0 %\n",
      "22m 54s (- 97m 23s) (3700 19%) loss : 8.304  accuracy : 4.0 %\n",
      "23m 31s (- 96m 46s) (3800 19%) loss : 8.304  accuracy : 3.9 %\n",
      "24m 8s (- 96m 8s) (3900 20%) loss : 8.314  accuracy : 4.3 %\n",
      "24m 45s (- 95m 31s) (4000 20%) loss : 8.263  accuracy : 4.1 %\n",
      "25m 22s (- 94m 52s) (4100 21%) loss : 8.274  accuracy : 4.2 %\n",
      "25m 59s (- 94m 16s) (4200 21%) loss : 8.245  accuracy : 4.0 %\n",
      "26m 36s (- 93m 38s) (4300 22%) loss : 8.229  accuracy : 4.2 %\n",
      "27m 13s (- 93m 1s) (4400 22%) loss : 8.219  accuracy : 4.0 %\n",
      "27m 50s (- 92m 23s) (4500 23%) loss : 8.226  accuracy : 4.1 %\n",
      "28m 26s (- 91m 44s) (4600 23%) loss : 8.193  accuracy : 4.3 %\n",
      "29m 3s (- 91m 6s) (4700 24%) loss : 8.168  accuracy : 4.2 %\n",
      "29m 41s (- 90m 30s) (4800 24%) loss : 8.182  accuracy : 4.2 %\n",
      "30m 17s (- 89m 52s) (4900 25%) loss : 8.169  accuracy : 4.2 %\n",
      "30m 54s (- 89m 14s) (5000 25%) loss : 8.153  accuracy : 4.3 %\n",
      "31m 31s (- 88m 35s) (5100 26%) loss : 8.178  accuracy : 3.8 %\n",
      "32m 8s (- 87m 58s) (5200 26%) loss : 8.131  accuracy : 4.3 %\n",
      "32m 45s (- 87m 21s) (5300 27%) loss : 8.125  accuracy : 4.2 %\n",
      "33m 21s (- 86m 43s) (5400 27%) loss : 8.132  accuracy : 4.1 %\n",
      "33m 58s (- 86m 5s) (5500 28%) loss : 8.132  accuracy : 4.2 %\n",
      "34m 35s (- 85m 26s) (5600 28%) loss : 8.104  accuracy : 4.2 %\n",
      "35m 11s (- 84m 48s) (5700 29%) loss : 8.099  accuracy : 4.3 %\n",
      "35m 48s (- 84m 11s) (5800 29%) loss : 8.094  accuracy : 4.3 %\n",
      "36m 27s (- 83m 37s) (5900 30%) loss : 8.073  accuracy : 4.4 %\n",
      "37m 4s (- 83m 0s) (6000 30%) loss : 8.087  accuracy : 4.3 %\n",
      "37m 41s (- 82m 23s) (6100 31%) loss : 8.045  accuracy : 4.3 %\n",
      "38m 17s (- 81m 45s) (6200 31%) loss : 8.036  accuracy : 4.3 %\n",
      "38m 54s (- 81m 7s) (6300 32%) loss : 8.029  accuracy : 4.3 %\n",
      "39m 31s (- 80m 29s) (6400 32%) loss : 8.027  accuracy : 4.4 %\n",
      "40m 8s (- 79m 52s) (6500 33%) loss : 8.037  accuracy : 4.5 %\n",
      "40m 45s (- 79m 15s) (6600 33%) loss : 8.021  accuracy : 4.4 %\n",
      "41m 22s (- 78m 38s) (6700 34%) loss : 8.011  accuracy : 4.3 %\n",
      "41m 59s (- 78m 0s) (6800 34%) loss : 8.006  accuracy : 4.3 %\n",
      "42m 35s (- 77m 22s) (6900 35%) loss : 8.016  accuracy : 4.3 %\n",
      "43m 12s (- 76m 44s) (7000 36%) loss : 7.976  accuracy : 4.5 %\n",
      "43m 48s (- 76m 7s) (7100 36%) loss : 7.988  accuracy : 4.5 %\n",
      "44m 25s (- 75m 29s) (7200 37%) loss : 7.987  accuracy : 4.3 %\n",
      "45m 2s (- 74m 51s) (7300 37%) loss : 7.979  accuracy : 4.4 %\n",
      "45m 38s (- 74m 14s) (7400 38%) loss : 7.965  accuracy : 4.4 %\n",
      "46m 15s (- 73m 37s) (7500 38%) loss : 7.943  accuracy : 4.6 %\n",
      "46m 52s (- 72m 59s) (7600 39%) loss : 7.954  accuracy : 4.5 %\n",
      "47m 29s (- 72m 22s) (7700 39%) loss : 7.960  accuracy : 4.3 %\n",
      "48m 6s (- 71m 45s) (7800 40%) loss : 7.918  accuracy : 4.5 %\n",
      "48m 43s (- 71m 8s) (7900 40%) loss : 7.925  accuracy : 4.7 %\n",
      "49m 20s (- 70m 31s) (8000 41%) loss : 7.926  accuracy : 4.7 %\n",
      "49m 57s (- 69m 54s) (8100 41%) loss : 7.926  accuracy : 4.5 %\n",
      "50m 34s (- 69m 17s) (8200 42%) loss : 7.936  accuracy : 4.5 %\n",
      "51m 10s (- 68m 39s) (8300 42%) loss : 7.914  accuracy : 4.6 %\n",
      "51m 47s (- 68m 2s) (8400 43%) loss : 7.917  accuracy : 4.4 %\n",
      "52m 24s (- 67m 25s) (8500 43%) loss : 7.901  accuracy : 4.6 %\n",
      "53m 1s (- 66m 48s) (8600 44%) loss : 7.905  accuracy : 4.6 %\n",
      "53m 38s (- 66m 11s) (8700 44%) loss : 7.889  accuracy : 4.5 %\n",
      "54m 15s (- 65m 34s) (8800 45%) loss : 7.890  accuracy : 4.6 %\n",
      "54m 51s (- 64m 56s) (8900 45%) loss : 7.899  accuracy : 4.5 %\n",
      "55m 29s (- 64m 19s) (9000 46%) loss : 7.890  accuracy : 4.7 %\n",
      "56m 5s (- 63m 42s) (9100 46%) loss : 7.863  accuracy : 4.6 %\n",
      "56m 42s (- 63m 5s) (9200 47%) loss : 7.898  accuracy : 4.6 %\n",
      "57m 19s (- 62m 28s) (9300 47%) loss : 7.848  accuracy : 4.7 %\n",
      "57m 56s (- 61m 50s) (9400 48%) loss : 7.871  accuracy : 4.6 %\n",
      "58m 33s (- 61m 13s) (9500 48%) loss : 7.860  accuracy : 4.6 %\n",
      "59m 10s (- 60m 37s) (9600 49%) loss : 7.828  accuracy : 4.8 %\n",
      "59m 46s (- 59m 59s) (9700 49%) loss : 7.809  accuracy : 4.8 %\n",
      "60m 23s (- 59m 22s) (9800 50%) loss : 7.837  accuracy : 4.8 %\n",
      "61m 1s (- 58m 46s) (9900 50%) loss : 7.843  accuracy : 4.8 %\n",
      "61m 38s (- 58m 9s) (10000 51%) loss : 7.850  accuracy : 4.7 %\n",
      "62m 15s (- 57m 32s) (10100 51%) loss : 7.843  accuracy : 4.7 %\n",
      "62m 52s (- 56m 55s) (10200 52%) loss : 7.821  accuracy : 4.7 %\n",
      "63m 28s (- 56m 18s) (10300 52%) loss : 7.808  accuracy : 4.6 %\n",
      "64m 5s (- 55m 41s) (10400 53%) loss : 7.789  accuracy : 4.8 %\n",
      "64m 43s (- 55m 4s) (10500 54%) loss : 7.821  accuracy : 4.7 %\n",
      "65m 19s (- 54m 27s) (10600 54%) loss : 7.797  accuracy : 4.9 %\n",
      "65m 56s (- 53m 50s) (10700 55%) loss : 7.792  accuracy : 4.9 %\n",
      "66m 33s (- 53m 12s) (10800 55%) loss : 7.786  accuracy : 5.0 %\n",
      "67m 10s (- 52m 36s) (10900 56%) loss : 7.781  accuracy : 4.9 %\n",
      "67m 47s (- 51m 58s) (11000 56%) loss : 7.795  accuracy : 4.7 %\n",
      "68m 24s (- 51m 21s) (11100 57%) loss : 7.766  accuracy : 5.0 %\n",
      "69m 1s (- 50m 44s) (11200 57%) loss : 7.770  accuracy : 4.8 %\n",
      "69m 38s (- 50m 7s) (11300 58%) loss : 7.783  accuracy : 4.8 %\n",
      "70m 15s (- 49m 30s) (11400 58%) loss : 7.758  accuracy : 4.9 %\n",
      "70m 51s (- 48m 53s) (11500 59%) loss : 7.791  accuracy : 4.9 %\n",
      "71m 28s (- 48m 16s) (11600 59%) loss : 7.773  accuracy : 4.9 %\n",
      "72m 5s (- 47m 39s) (11700 60%) loss : 7.733  accuracy : 5.2 %\n",
      "72m 42s (- 47m 2s) (11800 60%) loss : 7.750  accuracy : 4.8 %\n",
      "73m 19s (- 46m 25s) (11900 61%) loss : 7.739  accuracy : 4.9 %\n",
      "73m 56s (- 45m 48s) (12000 61%) loss : 7.741  accuracy : 4.7 %\n",
      "74m 33s (- 45m 11s) (12100 62%) loss : 7.715  accuracy : 4.9 %\n",
      "75m 9s (- 44m 34s) (12200 62%) loss : 7.754  accuracy : 4.9 %\n",
      "75m 46s (- 43m 57s) (12300 63%) loss : 7.742  accuracy : 5.0 %\n",
      "76m 23s (- 43m 20s) (12400 63%) loss : 7.745  accuracy : 4.8 %\n",
      "77m 0s (- 42m 43s) (12500 64%) loss : 7.733  accuracy : 4.8 %\n",
      "77m 37s (- 42m 6s) (12600 64%) loss : 7.684  accuracy : 5.2 %\n",
      "78m 14s (- 41m 29s) (12700 65%) loss : 7.724  accuracy : 5.0 %\n",
      "78m 51s (- 40m 52s) (12800 65%) loss : 7.717  accuracy : 4.9 %\n",
      "79m 27s (- 40m 15s) (12900 66%) loss : 7.703  accuracy : 5.0 %\n",
      "80m 4s (- 39m 38s) (13000 66%) loss : 7.733  accuracy : 5.1 %\n",
      "80m 41s (- 39m 1s) (13100 67%) loss : 7.677  accuracy : 5.1 %\n",
      "81m 18s (- 38m 24s) (13200 67%) loss : 7.710  accuracy : 5.1 %\n",
      "81m 54s (- 37m 47s) (13300 68%) loss : 7.688  accuracy : 5.0 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82m 31s (- 37m 10s) (13400 68%) loss : 7.688  accuracy : 5.1 %\n",
      "83m 8s (- 36m 33s) (13500 69%) loss : 7.724  accuracy : 4.9 %\n",
      "83m 46s (- 35m 56s) (13600 69%) loss : 7.698  accuracy : 5.0 %\n",
      "84m 23s (- 35m 19s) (13700 70%) loss : 7.696  accuracy : 5.1 %\n",
      "84m 59s (- 34m 42s) (13800 71%) loss : 7.677  accuracy : 5.2 %\n",
      "85m 36s (- 34m 5s) (13900 71%) loss : 7.665  accuracy : 5.1 %\n",
      "86m 13s (- 33m 28s) (14000 72%) loss : 7.714  accuracy : 5.0 %\n",
      "86m 50s (- 32m 51s) (14100 72%) loss : 7.670  accuracy : 5.2 %\n",
      "87m 27s (- 32m 14s) (14200 73%) loss : 7.676  accuracy : 5.0 %\n",
      "88m 4s (- 31m 37s) (14300 73%) loss : 7.669  accuracy : 5.0 %\n",
      "88m 41s (- 31m 0s) (14400 74%) loss : 7.701  accuracy : 5.2 %\n",
      "89m 18s (- 30m 23s) (14500 74%) loss : 7.674  accuracy : 5.0 %\n",
      "89m 55s (- 29m 46s) (14600 75%) loss : 7.678  accuracy : 5.0 %\n",
      "90m 31s (- 29m 9s) (14700 75%) loss : 7.675  accuracy : 4.9 %\n",
      "91m 8s (- 28m 32s) (14800 76%) loss : 7.623  accuracy : 5.2 %\n",
      "91m 45s (- 27m 55s) (14900 76%) loss : 7.648  accuracy : 5.2 %\n",
      "92m 22s (- 27m 18s) (15000 77%) loss : 7.658  accuracy : 5.1 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-1637cf6835c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mskipgram\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNgrams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.00025\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-30-283c08df4dd9>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, ngrams, iters, epochs, lr, random_state, print_every, compute_accuracy)\u001b[0m\n\u001b[0;32m    135\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mcouple\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mngrams\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m                     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainLoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcouple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m                     \u001b[0mtot_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                     \u001b[0mtot_loss_words\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-283c08df4dd9>\u001b[0m in \u001b[0;36mtrainLoop\u001b[1;34m(couple, optimizer, compute_accuracy)\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomputeAccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcompute_accuracy\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-283c08df4dd9>\u001b[0m in \u001b[0;36mcomputeAccuracy\u001b[1;34m(log_probs, targets)\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             acc = sum([log_probs[i, :, j].data.topk(1)[1].item() == targets[i, j].item() \n\u001b[1;32m---> 93\u001b[1;33m                        \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m                        for j in range(targets.size(1))])\n\u001b[0;32m     95\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-283c08df4dd9>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     92\u001b[0m             acc = sum([log_probs[i, :, j].data.topk(1)[1].item() == targets[i, j].item() \n\u001b[0;32m     93\u001b[0m                        \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m                        for j in range(targets.size(1))])\n\u001b[0m\u001b[0;32m     95\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "skipgram.train(Ngrams, epochs = 1, lr = 0.00025, print_every = 100, compute_accuracy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('olympic', 0.35984373),\n",
       " ('acceleration', 0.35875782),\n",
       " ('bizarre', 0.35553902),\n",
       " ('democracies', 0.3541008),\n",
       " ('literally', 0.3506152),\n",
       " ('manual', 0.35027972),\n",
       " ('temptation', 0.3372059),\n",
       " ('personal', 0.3276703),\n",
       " ('chiefs', 0.3203342),\n",
       " ('sedan', 0.318881)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar(word = 'looking', bound = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save & Load<br>\n",
    "\n",
    "The lightweight word2vec model can be saved for further use, or alternatively the full shell wrapping the word2vec model can be saved for subsequent training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "#torch.save(word2vec, path_to_DL4NLP + '\\\\saves\\\\DL4NLP_I1_skipgram.pt')\n",
    "\n",
    "# load\n",
    "#word2vec = torch.load(path_to_DL4NLP + '\\\\saves\\\\DL4NLP_I1_skipgram.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gensim\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Gensim Word2Vec\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Link : https://radimrehurek.com/gensim/models/word2vec.html<br>\n",
    "Tutorials :\n",
    "\n",
    "- https://cambridgespark.com/4046-2/\n",
    "- https://rare-technologies.com/word2vec-tutorial/\n",
    "- http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "\n",
    "### 1.2.1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath, get_tmpfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Training with CBOW objective\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Model & Data & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_gensim = Word2Vec(corpus_tokenized, \n",
    "                       size = 100, \n",
    "                       window = 5, \n",
    "                       min_count = 5, \n",
    "                       negative = 20, \n",
    "                       iter = 50,\n",
    "                       sg = 0,\n",
    "                       workers = multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('novice', 0.5856069326400757),\n",
       " ('amazed', 0.5641556978225708),\n",
       " ('searching', 0.5585623979568481),\n",
       " ('selling', 0.5566306710243225),\n",
       " ('writing', 0.5173830986022949),\n",
       " ('using', 0.4963681697845459),\n",
       " ('guessing', 0.48206374049186707),\n",
       " ('working', 0.4725167155265808),\n",
       " ('glad', 0.4712035655975342),\n",
       " ('hoping', 0.4707739055156708)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_gensim.wv.most_similar('looking')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save & Load\n",
    "\n",
    "The Gensim model can easily be saved & loaded :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "#file_name = get_tmpfile(path_to_DL4NLP + \"\\\\saves\\\\DL4NLP_I1_cbow_gensim.model\")\n",
    "#cbow_gensim.save(file_name)\n",
    "\n",
    "# load\n",
    "#file_name = get_tmpfile(path_to_DL4NLP + \"\\\\saves\\\\DL4NLP_I1_cbow_gensim.model\")\n",
    "#cbow_gensim = Word2Vec.load(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively it is direct to build a lightweight word2vec model out of a trained gensim model and then save & load it as done in previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = myWord2Vec(lang = Lang(corpus = [list(cbow_gensim.wv.index2word)], base_tokens = []), T = cbow_gensim.wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.most_similar('looking')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Training with SkipGram objective\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Model & Data & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_gensim = Word2Vec(corpus_tokenized, \n",
    "                           size = 100, \n",
    "                           window = 5, \n",
    "                           min_count = 5, \n",
    "                           negative = 20, \n",
    "                           iter = 50,\n",
    "                           sg = 1,\n",
    "                           workers = multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('novice', 0.6076338291168213),\n",
       " ('trying', 0.5377198457717896),\n",
       " ('getting', 0.531008243560791),\n",
       " ('working', 0.5226007699966431),\n",
       " ('going', 0.5183506011962891),\n",
       " ('interested', 0.5151867270469666),\n",
       " ('need', 0.5117555856704712),\n",
       " ('sale', 0.5094774961471558),\n",
       " ('planning', 0.5061888098716736),\n",
       " ('wizard', 0.5041382312774658)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgram_gensim.wv.most_similar('looking')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "#file_name = get_tmpfile(path_to_DL4NLP + \"\\\\saves\\\\DL4NLP_I1_skipgram_gensim.model\")\n",
    "#skipgram_gensim.save(file_name)\n",
    "\n",
    "# load\n",
    "#file_name = get_tmpfile(path_to_DL4NLP + \"\\\\saves\\\\DL4NLP_I1_skipgram_gensim.model\")\n",
    "#skipgram_gensim = Word2Vec.load(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sub_word_level\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2 Word Embedding via sub-word units\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"fastText\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 FastText's Word Embedding via character n-grams\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "\n",
    "We consider the Gensim implementation of FastText, based on the CBOW training objective.<br>\n",
    "Tutorial : [Gensim FastText](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb)<br>\n",
    "Link to the original paper : [Enriching Word Vectors with Subword Information](https://arxiv.org/pdf/1607.04606.pdf).\n",
    "\n",
    "### 2.1.1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText as FT_gensim\n",
    "from gensim.test.utils import datapath, get_tmpfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Training with CBOW objective\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Model & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_fastText_gensim = FT_gensim(size = 100, \n",
    "                                 window = 5, \n",
    "                                 min_count = 5, \n",
    "                                 negative = 20,\n",
    "                                 sg = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_fastText_gensim.build_vocab(corpus_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_fastText_gensim.train(sentences = corpus_tokenized, \n",
    "                           epochs = 50,\n",
    "                           total_examples = cbow_fastText_gensim.corpus_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hooking', 0.864661693572998),\n",
       " ('overlooking', 0.855728268623352),\n",
       " ('cooking', 0.8507105708122253),\n",
       " ('joking', 0.8126223683357239),\n",
       " ('smoking', 0.753277599811554),\n",
       " ('-king', 0.7203484773635864),\n",
       " ('seeking', 0.7179160118103027),\n",
       " ('searching', 0.715704083442688),\n",
       " ('idling', 0.7143186330795288),\n",
       " ('scanning', 0.7119997143745422)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_fastText_gensim.wv.most_similar('looking')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "#file_name = get_tmpfile(path_to_DL4NLP + \"\\\\saves\\\\DL4NLP_I1_cbow_fasttext.model\")\n",
    "#cbow_fastText_gensim.save(file_name)\n",
    "\n",
    "# load\n",
    "#file_name = get_tmpfile(path_to_DL4NLP + \"\\\\saves\\\\DL4NLP_I1_cbow_fasttext.model\")\n",
    "#cbow_fastText_gensim = FT_gensim.load(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively it is direct to build a lightweight word2vec model out of a trained gensim model and then save & load it as done in previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = myWord2Vec(lang = Lang(corpus = [list(cbow_fastText_gensim.wv.index2word)], base_tokens = []), T = cbow_fastText_gensim.wv.vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the main advantage FastText offers is the possibility to get an embedding vector out of **any word**, and in fact any string thanks to the character-ngrams embedding trick :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_fastText_gensim['HelloWorld']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonetheless, it can be interesting to load the look-up word vectors table into a lightweight word2vec module, as it allows to further optimize this table for any specific downstream task performed by a larger PyTorch model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Training with SkipGram objective\n",
    "\n",
    "[Back to top](#plan)\n",
    "\n",
    "Model & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_gensim = FT_gensim(size = 100, \n",
    "                           window = 5, \n",
    "                           min_count = 5, \n",
    "                           negative = 20,\n",
    "                           sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_gensim.build_vocab(corpus_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_gensim.train(sentences = corpus_tokenized, \n",
    "                      epochs = 50,\n",
    "                      total_examples = fastText_gensim.corpus_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('novice', 0.6242992877960205),\n",
       " ('hooking', 0.5955933928489685),\n",
       " ('getting', 0.5934851169586182),\n",
       " ('researching', 0.5598569512367249),\n",
       " ('converting', 0.5522629022598267),\n",
       " ('marching', 0.5496691465377808),\n",
       " ('designing', 0.5489811897277832),\n",
       " ('buying', 0.5446159839630127),\n",
       " ('searching', 0.5296090245246887),\n",
       " ('pd', 0.5229203104972839)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastText_gensim.wv.most_similar('looking')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "#file_name = get_tmpfile(path_to_DL4NLP + \"\\\\saves\\\\DL4NLP_I1_skipgram_fasttext.model\")\n",
    "#fastText_gensim.save(file_name)\n",
    "\n",
    "# load\n",
    "#file_name = get_tmpfile(path_to_DL4NLP + \"\\\\saves\\\\DL4NLP_I1_skipgram_fasttext.model\")\n",
    "#fastText_gensim = FT_gensim.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fastText_gensim[['13', 'to']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
